---
title: "Taking a spatial perspective on the `radon` data"
author: "Jon Zelner"
date: "2/28/2023"
image: "minnesota_uranium.png"
draft: false 
---
```{r setup}
#| warning: false
#| echo: false
library(ggplot2)
library(arm)
library(tidycensus)
library(dplyr)
library(rstanarm)
library(stringr)
library(spdep)
```

# Introduction

This tutorial is a follow-up to a prior exercise using these data. So if you haven't already, please go back and take a look at the original multi-level modeling radon example [here](/posts/radon/index.html).

In order to work with these data, we'll first need to merge the original radon measurements into a shapefile for the state of Minnesota so that we can visualize and analyze spatial patterning of our key exposure of interest (soil uranium) as well as the outcome of interest, i.e. household-level radon, as well as indicators of goodness-of-fit including the model residuals.

### Learning Goals

The primary goals of this tutorial are to introduce you to:

1. Merging of non-spatial health exposure or outcome data with spatial metadata.
2. Calculation of important spatial summary statistics, e.g. Moran's I, from such data.
3. Spatial analysis of residuals from aspatial regression models of spatially-referenced data.

## Data Preparation

Before diving into the analysis steps, there are several key things we need to do to be able to easily work with these data:

### Download a shapefile for Minnesota

First, we need to get a shapefile for the state of Minnesota in which each polygon represents an individual county. Thankfully, in R, this is made easy using the excellent `tidycensus` package:

```{r}
options(tigris_use_cache = TRUE)

minnesota <- get_acs(
  state = "MN",
  geography = "county",
  variables = "B19013_001",
  geometry = TRUE,
  year = 2020
)
```

Tidycensus gives us the data as an `sf` dataframe containing a number of fields including population estimates, which we can plot straightforwardly using the `plot` function supplied by the `sf` package:

```{r}
plot(minnesota["estimate"])
```
### Merge the spatial data with the radon data

In its raw form, this spatial dataset isn't quite ready to merge with the radon data. If we take a peek at the county names in the shapefile, we can see that they don't quite match the formatting of the ones in the original data:

```{r}
head(sort(minnesota$NAME))
```

Whereas in the radon data we see:

```{r}
head(unique(as.character(radon$county)))
```

The big differences here are that the shapefile uses: 1) mixed-case county names and 2) includes the name of the state in each label. To make these match the `radon` dataset, we can use some tools from the `stringr` package as well as some base R functions: 
```{r}
minnesota <-
  minnesota %>% mutate(
    ## Since all of the original county names have the same substring " County, Minnesota"
    ## we can use the str_remove function to pull them out of all of them
    county = str_remove(NAME, " County, Minnesota") %>% 
      ## Since some of the counties  officially have two-word names (e.g. Big Stone)
      ## which are collapsed in the radon dataset, we will use this function to remove all spaces:
      str_replace_all(" ", "") %>% 
      ## A few county names include abbreviations indicated by the presence of a '.' (e.g. St. Louis)
      ## so we will get rid of that bit of punctuation since it is not in the original data
      str_replace_all("\\.", "") %>% 
      ## Finally, convert all the county names to uppercase
      toupper()
  )

```
Now, the county labels should match:
```{r}
head(sort(minnesota$county))
```

### Preparing the `radon` dataset

We will repeat the steps from the earlier tutorial in order to prepare our data for analysis:

```{r}
radon <- radon %>% mutate(basement = 1 - floor)

county_uranium <- radon %>%
  group_by(county) %>%
  summarize(log_uranium = first(log_uranium), 
            median_radon = median(log_radon))
```

Because the sf dataset returned by tidycensus is a dataframe, we can then easily merge the county-level soil uranium concentrations we derived above into the shapefile. We use the `left_join` function from `dplyr` to ensure that all of the counties in the original shapefile are represented in the final dataset, even if a soil uranium measure is unavailable for them in the original data:

```{r}
minnesota_radon <- left_join(minnesota, county_uranium)
```

We can then plot the log-uranium measures on the map and see that, in fact, they are quite spatially correlated. We can also see that there appear to be two counties which are missing soil uranium data in the `radon` dataset:

```{r}
plot(minnesota_radon["log_uranium"])
```

## Measuring Spatial Correlation

To validate our hunch that soil uranium is spatially concentrated in Minnesota, we can calculate the value of Moran's I for these data using some functions from the `spdep` package. First, we use the `poly2nb` function to obtain the neighbors for each polygon, which will be used to calculate Moran's I.  

```{r}
nb <- poly2nb(minnesota_radon)
```

This function yields an R list in which each entry is a vector with the indices for the neighbors of the i-th county. For example, this prints the neighbors of the first three counties in the dataset:

```{r}
#| echo: false
#| warning: false
print(nb[1:3])
```

We then pass this function to the `nb2listw` function to obtain weights for the relationships between neighbors. Here, we use the simplest option available, "B", for binary weights equal to 1 if the areas are neighbors and 0 otherwise:
```{r}
lw <- nb2listw(nb, style="B", zero.policy=TRUE)
print(lw$weights[1:3])
```

Finally, we can pass these weights, along with some additional information including the outcome of interest at each location, the total number of locations, and the sum of all the weights to the `moran` function. The `NAOK=TRUE` option used here also allows the function to drop locations where data are missing:
```{r}
radon_i <- moran(minnesota_radon$log_uranium, lw, length(nb), Szero(lw),NAOK=TRUE)$I
```

When we do this, we find that the value of Moran's I = `r round(radon_i,2)`, which is close to the maximum value of 1. Since we'll be returning to the calculation of Moran's I using our spatial data, lets pack it up into a function:

```{r}
#| warning: false
moranFromSF <- function(x, sfdf, style="B") {
  nb <- poly2nb(sfdf)
  lw <- nb2listw(nb, style=style, zero.policy=TRUE)
  mi <- moran(x, lw, length(nb), Szero(lw), NAOK=TRUE)$I
  return(mi)
}

print(moranFromSF(minnesota_radon$log_uranium, minnesota_radon))
```

Of course, our key quantity of interest isn't soil uranium but the concentration of radon at the household level. When we constructed the `county_uranium` dataset above, we also calculated the median radon concentration in the data for each county. When we plot it, we see something similar to the soil uranium, but perhaps a bit less clear:

```{r}
plot(minnesota_radon["median_radon"])
```

Using our function for computing Moran's I, we can easily go ahead and determine how clustered these data are:
```{r}
radon_mi <- moranFromSF(minnesota_radon$median_radon, minnesota_radon)
```

Which comes out to `r round(radon_mi, 2)`, which is smaller than the value we got for uranium, but still indicates meaningful clustering. 

:::{.callout-tip}
## What's going on?
Pause here and take a moment to try to figure out what might account for the difference in this intensity of clustering? 
:::

### Testing, testing

One way to determine whether the spatial aggregation of the radon measurements is meaningful is to compare it to a *counterfactual scenario* in which the distribution of radon concentrations is known to be uncorrelated with space. This assumption, known as complete spatial randomness (or CSR), allows us to provide a benchmark against which we determine whether the value of Moran's I we determined is highly likely to occur by chance alone. Thankfully, it is easy to generate a dataset in which the median radon values are distributed randomly across the map:

```{r}

## Make a new dataset representing 'random minnesota'
random_minnesota <- minnesota_radon

## Use the sample function to resample median radon values without replacement,
## which yields a vector in which the values of median radon are simply shuffled
## by county
random_minnesota$median_radon <- sample(minnesota_radon$median_radon, nrow(minnesota_radon), replace = FALSE)

## Plot the new randomized data
plot(random_minnesota["median_radon"])
```
This yields something that looks pretty randomly distributed, and when we calculate Moran's I, we get a value of `r round(moranFromSF(random_minnesota$median_radon, random_minnesota),2)`, which is closer to the null value of 0. But this still doesn't tell us anything. What we can do, though, is to generate a bunch of random minnesotas and calculate moran's I for each of those and see how our observed data stack up.

```{r}
#| warning: false
csrMorans <- function(x, sfdf, trials = 1000, style="B") {
  nb <- poly2nb(sfdf)
  lw <- nb2listw(nb, style=style, zero.policy=TRUE)
  
  moran_vals <- rep(0, trials)
  for (i in 1:trials) {
    random_x <- sample(x, length(x), replace=FALSE)
    moran_vals[i] <- moran(random_x, lw, length(nb), Szero(lw),NAOK=TRUE)$I
  }
  
  return(moran_vals)
}  

csr_dist <- csrMorans(minnesota_radon$log_uranium, minnesota_radon)
```

This yields a distribution with a median of `r round(median(csr_dist))`, and the empirical cdf function shows us the approximate probability of obtaining a value of Moran's greater than or equal to our observed value by random chance alone:

```{r}
moran_dist <- ecdf(csr_dist)
plot(moran_dist)
```

And we can directly estimate this probability as follows:
```{r}
#| warning: false
real_moran <- moranFromSF(minnesota_radon$median_radon, minnesota_radon)
p_moran <- sum(csr_dist >= real_moran)/length(moran_dist)
print(p_moran)
```

From 1000 samples, it appears that none of our random datasets yielded a value of Moran's I $\ge$ to the observed value, suggesting that it is unlikely that we would observe this value as a simple function of sampling variability. 

::: {.callout-warning}
## What could go wrong? 

Before you move on, take a minute to think about what some of the potential flaws in our CSR-based approach to assessing the meaningfulness or signficance of this result might be.
:::

