{
  "hash": "3c3eaed8c754cc986a5a7e80d0a64c13",
  "result": {
    "markdown": "---\nauthor: \"Jon Zelner\"\ndate: \"1/17/2023\"\ncategories:\n  - code\n  - tutorial\n---\n\n\n# Codifying Tobler's First Law using Locally Weighted Regression\n\nIn this brief tutorial, we will review some basic ideas about smoothing and start thinking through how we can express these ideas mathematically and in R. \n\nTobler's profound - but deceptively simple - first law states that:\n\n> \"Everything is related to everything else. But near things are more related than distant things.\" [@tobler1970]\n\nHe applied this idea to his development of a dynamic model of urban growth in the Detroit region which assumed that rates of population growth were spatially similar:\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/kRsF9S8JqBI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\nIn this post, we're going to start with a simpler problem - change in the value of a function in one dimension - to see how we can translate the concept of *distance decay* implied by Tobler's first law (TFL) into a useful model. To begin, we're going to keep it simple with no noise or observation error and just interpolate some values.\n\n## Notation and Terminology\n\nIn this example, we are interested in visualizing and predicting the values of a function $f(x_i)$ which outputs values y_i, the expected value of the output function.\n\n$$\ny_i = f(x)\n$$\n\n\nLets start by getting the values of $f(x)$ for every input value $x$. For simplicity, we will assume that $f(x)$ is a sine function and that the values of $x$ go from -1 to +5, allowing us to observe one half cycle of the sine function: \n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(-1, 5, by = 0.1)\ny <- sin(x)\nplot(x,y)\n```\n\n::: {.cell-output-display}\n![](smoothing_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n \nYou can see right away that this simple curve pretty neatly expresses Tobler's first law: $f(x)$ values of each point are in general more similar to each other for nearby values of $x$. If we want to press this idea into real-world practice, we need a *model* that can translate TFL into quantitative estimates and qualitative visualizations. There are lots of ways to do this, but we'll focus in on  locally-weighted regression, also known LOWESS. \n\nThe basic idea of a LOWESS regression is to define a window of size $k$ points around each value one wishes to estimate, and calculate a weighted average of the value of those points, which can then be used as the estimated value $\\hat{y_j} \\approx f(x_j)$. We then run the values of these nearest neighbors through a weight function $w(x)$. \n\nThese weight functions can take a lot of different forms, but we'll start simple with a uniform one, i.e. just taking the average of the $k$ nearest neighbors, so that $\\hat{y} = sum(z(x_i, k))/k$, where $KNNz$ is a function returning the $y$ values of the k nearest observations to $x_i$. The value of $k$ is sometimes referred to as the *bandwidth* of the smoothing function: Larger bandwidths use more data to estimate values at each point, smaller ones use less. \n\n### Making some locally weighted estimates\n\nUsing the `fnn` package for R, we can find the indices of the $k$ nearest neighbors of each point we want to make an estimate at:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(FNN)\nk <- 10\nz <- knn.index(x, k=k)\n```\n:::\n\n\nYou can read the output of this function, below, as indicating the indices ($i$) of the 10 nearest points to each of the values of $x$.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    2    3    4    5    6    7    8    9   10    11\n [2,]    1    3    4    5    6    7    8    9   10    11\n [3,]    2    4    1    5    6    7    8    9   10    11\n [4,]    5    3    6    2    1    7    8    9   10    11\n [5,]    6    4    7    3    8    2    1    9   10    11\n [6,]    5    7    4    8    3    9    2   10    1    11\n [7,]    8    6    9    5   10    4   11    3   12     2\n [8,]    7    9   10    6   11    5    4   12    3    13\n [9,]   10    8   11    7   12    6    5   13   14     4\n[10,]    9   11    8   12    7   13   14    6    5    15\n```\n:::\n:::\n\n\nWe can visualize this by picking a point in the middle of the series and its 10 nearest neighbors and the estimated value of $\\hat{y_i}$ obtained by just taking the average of the k nearest points:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n## Plot the original data\ng <- ggplot() + geom_point(aes(x=x, y = y)) + \n  xlab(\"x\") + ylab(\"f(x)\")\n\n## Now, get the index for x = 2\nx_index <- which(x==2)\n\n## Show the range of k nearest neighbors of this point\nknn_low <- min(x[z[x_index, ]])\nknn_high <- max(x[z[x_index, ]])\ny_hat <- mean(y[z[x_index,]], )\n\n## Add errorbars to the figure to show the 10 nearest values with the height of the point indicating the estimated value at y_i, denoted by the red dot\ng <- g + geom_errorbarh(aes(xmin = knn_low, xmax = knn_high, y = y_hat)) + geom_point(aes(x=2,y_hat), colour = \"red\")\n\nplot(g)\n```\n\n::: {.cell-output-display}\n![](smoothing_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\nNotice that if the `knn` function is applied at the low end of the series, i.e. to the first value, it will use points to the right of that one instead of to either side:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n## Plot the original data\ng <- ggplot() + geom_point(aes(x=x, y = y)) + \n  xlab(\"x\") + ylab(\"f(x)\")\n\n## Use the index for the lowest value\nx_index <- 1\n\n## Show the range of k nearest neighbors of this point\nknn_low <- min(x[z[x_index, ]])\nknn_high <- max(x[z[x_index, ]])\ny_hat <- mean(y[z[x_index,]], )\n\n## Add errorbars to the figure to show the 10 nearest values with the height of the point indicating the estimated value at y_i\ng <- g + geom_errorbarh(aes(xmin = knn_low, xmax = knn_high, y = y_hat)) + geom_point(aes(x=x[x_index],y_hat), colour = \"red\")\n\nplot(g)\n```\n\n::: {.cell-output-display}\n![](smoothing_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nNow, lets see what happens if we run our smoother over the whole series and take the average of the 10 nearest points for each and compare them to the observed data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_hat <- rep(0, length(x))\nfor (i in 1:length(x) ) {\n  y_hat[i] <- mean(y[z[i,]], )\n}\n```\n:::\n\n\nNow plot the prediucted vs. the observed values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- ggplot() + geom_point(aes(x=x, y = y)) + \n  xlab(\"x\") + ylab(\"f(x)\") + geom_line(aes(x=x, y=y_hat), colour = \"red\")\nplot(g)\n```\n\n::: {.cell-output-display}\n![](smoothing_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nYou can see this does a pretty good job all the way through, except at the edges. Lets try it again with a smaller window - or bandwidth - of 5 and see what happens. First, we'll write a function that will give us the predicted value of y at each point given a window of size k and an input value: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_est <- function(x, y, k) {\n  z <- knn.index(x, k=k)\n  y_hat <- rep(0, length(x))\n  \nfor (i in 1:length(x) ) {\n  y_hat[i] <- mean(y[z[i,]], )\n}\n\n  df <- data.frame(x=x, y = y, yhat = y_hat)\n  return(df)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npred_df <- knn_est(x, y, 5)\ng <- ggplot(pred_df) + geom_point(aes(x=x, y=y)) + geom_line(aes(x=x,y=yhat),colour=\"red\")\nplot(g)\n```\n\n::: {.cell-output-display}\n![](smoothing_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThis gets rid of a lot of the weird effects at the edges but introduces some noise into the function. What if we make the window bigger, say 20, to get rid of some of the noise?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](smoothing_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThis seems to make the edge effects worse, as well as the estimates of the function overall worse.\n\nWhat happens if we go in the opposite direction and shrink the window down to 2?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](smoothing_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n### Discussion Questions\n\n- Why does this appear to be more accurate for these data than $k=10$ and $k=5$? \n\n- What would happen if we added observation noise to the values of $y_i$? Which one of the smoothers do you think would work better then?\n\n- Is the one *best* value of $k$ for all datasets? How might you go about picking the best one? \n\n- How does our uniform weight function express Tobler's first law? What kind of weight function $w(x)$ might do a better job of capturing the notion of distance decay?\n\n## References\n\n",
    "supporting": [
      "smoothing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}