[
  {
    "objectID": "epid594/index.html",
    "href": "epid594/index.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Welcome to the companion site to EPID 594, “Key Concepts in Spatial Analysis”, at the University of Michigan School of Public Health. This is an online course that includes four synchronous zoom sessions and a project that will allow you to develop your skills and knowledge in an area of spatial epidemiology that is of intellectual or professional interest to you.\n\n\n\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\n2/14\nSmoothing!\n\n\n2/21\nHierarchical models\n\n\n2/28\nSegregation and the spatial epidemiology of infectious disease\n\n\n3/7\nWhat’s next in spatial epidemiology?\n\n\n\n\n\n\nThe goal of the project for this course is to advance either or both your skill in spatial analysis and/or your knowledge of a particular topic area in spatial epidemiology. Given that we have a short period of time in which to complete this, I would encourage you to build on something you already are working on or an existing interest. The project consists of three components:\n\nProject Proposal: A short (2-3 page) proposal for what you would like to do, which includes some background on the topic of your project, its relevance to the field of spatial epidemiology, and some information about the goals of your project. Specifically, I am asking you to articulate:\n\nWhat is the problem you are tackling?: What is the problem you are proposing to focus on? Are you interested in a particular health outcome, a system of exposure (i.e. an environmental exposure or a social system such as racism/economic inequality?), or something else? Provide some evidence to show why developing a better understanding of how this system works and/or how to intervene in it is an important public health priority.\nProject Learning Goal: What would you like to learn through completing your project? It could be a statistical analysis approach, mapping skill, experience with a particular tool or framework (i.e. Shiny for R), or to learn more about the spatial dimensions of a public health topic of interest. Identify your key goal and briefly explain why it is important to you from an intellectual/academic and professional perspective.\nProposed project format: What do you anticipate producing for your project? It could be a research paper, a detailed data visualization, a map, an interactive app, or something eld`se entirely. Think about an output format that aligns with your identified learning goals. What could you produce that would be helpful for you in your professional or academic career, either for showcasing skills or pushing a project forward?\n\nProject Update: A brief (1-2 page) update on where you are in your project work and an opportunity to ask for feedback and/or have specific questions answered. Specifically, answer the following questions:\n\nWhat have you found most interesting or surprising so far in your investigations?\nWhat is confusing or something you need help thinking through?\nWhat modifications are you making to your project goals/outputs in response to what you have learned through your research so far?\n\nFinal Product: At the end of Week 4, I am asking you to submit your final product in whatever format and stage of completeness it is at.\n\nSelf-assessment: Along with your final product, please provide a short (~1 page) assessment of your progress towards the goals you set at the beginning of the project. A successful project doesn’t have to have hit the entire goal, but instead can be one that has either a) made progress towards it or b) highlighted that another goal is more worthy/important. Based on your self-assessment, please also suggest a letter grade (incl -/+, i.e. B+, A-, A) for the course. Unless your self-assessment is far out of line with my own, I will defer to your self-assessed grade."
  },
  {
    "objectID": "epid594/session_1_smoothing.html",
    "href": "epid594/session_1_smoothing.html",
    "title": "Zelnotes",
    "section": "",
    "text": "In our first Zoom session of this course, we will spend some time discussing and thinking about the concept of spatial smoothing and why it is such a central idea in spatial analysis and medical geography. Specifically, we will spend our time on the following activities:\n\nIntroductions/logistics (~10m)\nTime for feedback/questions about readings and course materials. (~10m)\nA mini-lecture on the concept of smoothing and why it is important (~10m)\nA conceptual introduction to kernel smoothing\nShort break (~5m)\nA short conceptual introduction to kernel density estimation in one dimension (~10m)\nIf time: A hands-on tutorial about way smoothers are employed to make estimates and identify hotspots.(~25m)\nWrap-up/housekeeping for next week.\n\n\n\n\nLecture slides\nAnother brief tutorial on smoothing continuous data in one dimension"
  },
  {
    "objectID": "epid594/session_2_multilevel.html",
    "href": "epid594/session_2_multilevel.html",
    "title": "Zelnotes",
    "section": "",
    "text": "For our second zoom session, we’re going to do a bit of a deep dive, hands-on take on multi-level modeling.\n\n\nIn class, we will be doing a walk-through of how to reproduce the analysis in Gelman (2006) using several R-based modeling tools. Before class, please:\n\nInstall or update to the latest version of RStudio. The tutorial code will be contained in a Quarto markdown document. Quarto is an updated version of the venerable RMarkdown, and the newest versions of RStudio include Quarto support by default.\n\nBefore or at the beginning of class:\n\nSet up your R/RStudio installation to be able to load the following packages using the following code:\n\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(bayesplot)\nlibrary(rstanarm)\nlibrary(purrr)\nlibrary(tidybayes)\n\nIf you are not sure if you have these installed or want to update to the latest versions, please paste this command into a running R session to download and install:\n\ninstall.packages(c(\"ggplot2\",\"tidyr\",\"dplyr\",\"bayesplot\",\"rstanarm\",\"purrr\",\"tidybayes\"))\n\nIf you run into problems with any of these, please let me know!\n\n\n\n\nSlide\nYou can find the tutorial we are going to work through here\n\n\n\n\n\nZoom recording"
  },
  {
    "objectID": "epid594/session_3_segregation.html",
    "href": "epid594/session_3_segregation.html",
    "title": "Zelnotes",
    "section": "",
    "text": "In this session, we will focus on the quantitative complement to the more-qualitative analysis of residential segregation and infectious disease presented in (Roberts 2009).\nSpeficially, during class we will briefly discuss the readings as well as some of the key measures used to characterize residential segregation. We will complete the following two hands-on exercises (time-permitting):\n\nComparing measures of spatial segregation and clustering.\nExamining the mechanistic implications of residential segregation for infection inequity. (If time ⏳)\n\n\n\n\nSlides\nZoom recording"
  },
  {
    "objectID": "epid594/session_4_whats_next.html",
    "href": "epid594/session_4_whats_next.html",
    "title": "Zelnotes",
    "section": "",
    "text": "In this session, we will focus on understanding what is around the bend in spatial epidemiology and how to make it most relevant to real-world practice and applied research.\nWe will also complete a second hands-on activity taking a more spatial perspective on the radon data from Gelman (2006).\n\n\n\nSlides"
  },
  {
    "objectID": "epid684/class_1_where are we going.html",
    "href": "epid684/class_1_where are we going.html",
    "title": "Zelnotes",
    "section": "",
    "text": "In this first session, we will spend the first 30 minutes going over the goals of the course, the components of the semester-long Roadmap Project, and start to discuss what we actually mean when we talk about spatial epidemiology. \n\n\n\n\nSlides\nZoom recording of class session\nGoogle doc for in-class activity.\n“What is spatial epidemiology anyway?” short recorded lecture."
  },
  {
    "objectID": "epid684/class_2_maps_and_more.html",
    "href": "epid684/class_2_maps_and_more.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Please take some time to read over this excellent introduction to some of the key questions and challenges in spatial epidemiology:\nKirby RS, Delmelle E, Eberth JM. Advances in spatial epidemiology and geographic information systems. Annals of Epidemiology. 2017;27(1):1-9. doi:10.1016/j.annepidem.2016.12.001\nAs well as this piece, which reviews some of the social and technological innovations in spatial epidemiology that have occurred in the wake of the COVID-19 pandemic:\nKoch T. Welcome to the revolution: COVID-19 and the democratization of spatial-temporal data. PATTERNS. 2021;2(7). doi:10.1016/j.patter.2021.100272 (Open Access)\n\n\n\nFor the first half of class, we will go over the components of the [[Roadmap Project]] in detail and discuss any questions you might have about the readings for today.\nIn the second half of class, we will complete the [[Good Map, Bad Map]] activity, in which you will look for examples of contemporary maps that do and do not effectively convey spatial information. We will use this as a springboard for discussing the power and limitations of maps as presentations of cause-and-effect in epidemiology and beyond. Please use the assigned readings as a guide for this conversation, as they get into some of the tools available for conducting spatial epidemiology and some of the key ideas underlying the health geography perspective.\n\n\n\nLecture slides\nZoom recording of class session\nGoogle doc for in-class activity\nSlides for in-class activity deck for in-class activity\n“Making maps means making choices” short recorded lecture\n“Tools of the trade” short recorded lecture\nThis chapter provides a comprehensive overview of important ideas and topics in spatial epidemiology: Robb SW, Bauer SE, Vena JE. Chapter 1: Introduction to Different Epidemiologic Perspectives and Applications to Spatial Epidemiology. In: Handbook of Spatial Epidemiology. CRC Press; 2016:4-18."
  },
  {
    "objectID": "epid684/class_3_relationships.html",
    "href": "epid684/class_3_relationships.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Spatial Epidemiology is All About Relationships\nThe focus of this class session will be on understanding the concept of spatial autocorrelation and variation and what they means for epidemiological analysis. Where variation is large, we should expect to see big differences from place to place.\nAutocorrelation is about how that variation plays out over space: Where there is strong spatial autocorrelation, things that are close to each other are far more likely to be similar than things that are far away from each other. But one can have strong autocorrelation and low variation, high variation and no autocorrelation, and on and on…\nWe will spend this class session trying to make sense of this concept in qualitative terms, first by discussing a pair of articles that nicely lay out the key ideas here, and then by doing a hands-on activity focused on visualizing patterns of spatial variation and autocorrelation using a spatial smoothing approach.\n\nBefore Class\nPlease read the following two pieces that discuss key ideas about geospatial relatedness:\nMiller HJ. Tobler’s First Law and Spatial Analysis. Annals of the Association of American Geographers. 2004;94(2):284-289. doi:10.1111/j.1467-8306.2004.09402005.x\nGoodchild MF. The Validity and Usefulness of Laws in Geographic Information Science and Geography. Annals of the Association of American Geographers. 2004;94(2):300-303. doi:10.1111/j.1467-8306.2004.09402008.x\n\n\nDuring Class\n\nWe will discuss Tobler’s “first law” of geography and the proposed alternative “laws” in the Goodchild paper.\nIn the second half of class, we will complete a hands-on exercise focused on introducing the concept of smoothing, which is an important tool in spatial analysis for visualizing and understanding the causes of patterns of spatial autocorrelation.\n\n\n\nAdditional Resources\n\nSlides\nZoom recording of class session\n“Some Laws of Health Geography” - pre-recorded video introducing the topics of spatial autocorrelation and spatial variation."
  },
  {
    "objectID": "epid684/class_4_spatial_variation.html",
    "href": "epid684/class_4_spatial_variation.html",
    "title": "Zelnotes",
    "section": "",
    "text": "In this session, we will focus in on the problem of how we can use epidemiological data to examine the role of Tobler’s first law - the universality of spatial relatedness - as well as Goodchild’s revised first law - the ubiquity of spatial variation - in epidemiology data.\n\n\nPlease read the following two selections. The first is a short article introducing the concept of a disease ‘hotspot’ and the second is a more in-depth treatment of the idea of disease clustering:\n\nRead the piece “What is a Hotspot, Anyway?” by Lessler et al. (2017)\nAnd Chapter 3: Interpreting Clusters of Disease Events from (Lawson et al. 2016)\n\n\n\n\nDuring the first half of class, we will discuss the readings and key concepts in them and I will introduce the concept of kernel smoothing and its application to epidemiologic data as a tool for visualizing and identifying disease clusters.\nIn the second half of class, we will complete a guided tutorial going over the basics of locally-weighted regression and, if we have time, a hands-on activity in which we will experiment with smoothing different types of simulated data using this interactive tutorial.\n\n\n\nSlides\nZoom recording of class session"
  },
  {
    "objectID": "epid684/class_5_walking_into_a_cholera_epidemic.html",
    "href": "epid684/class_5_walking_into_a_cholera_epidemic.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Read:\nThe Ghost Map, (Preface, Chs 1 & 2, pp. vx-55) (Johnson 2007)\n\n\n\nWe will begin discussing one of the foundational stories of modern infectious disease epidemiology: The 1854 London Cholera Outbreak. We will focus first on understanding what this outbreak looked like: What happens when you get Cholera, how it is spread, who is most vulnerable, etc.\nWe will also begin making some connections between the historical example laid out in the book and the very real challenges that remain in combating Cholera today, and in communicating new scientific ideas to a not-always-receptive audience.\n\n\n\n\nLecture Slides\nZoom recording"
  },
  {
    "objectID": "epid684/class_8_miasmas_mental_models.html",
    "href": "epid684/class_8_miasmas_mental_models.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Today’s class is focused on taking the ideas from The Ghost Map into the present.\n\n\n\n\nWe will read one paper that gets into direct lessons from the Broad Street Outbreak (Greenhalgh 2021), and a short opinion piece discussing the relationship between social disparities and spatial analysis (Chowkwanyun and Reed 2020). (Both are open-access and should be downloadable directly from the journal site.)\n\n\n\n\nWe will finish our exploration of the concept of smoothing with a hands-on tutorial of hotspot detection using kernel density estimation in two dimensions.\n\n\n\n\nSlides\nZoom recording"
  },
  {
    "objectID": "epid684/index.html#course-goals",
    "href": "epid684/index.html#course-goals",
    "title": "Zelnotes",
    "section": "Course Goals",
    "text": "Course Goals\nIn this course, we will take an exploratory and collaborative approach to defining and understanding the key problems and approaches to spatial problems in epidemiology and public health.  Through reading historical and contemporary examples of spatial analysis in public health, completing hands-on labs that give a sense of how key mechanisms and methods work, and pursuing an independent research project, you be able to identify the most important biological, social and environmental mechanisms that drive spatial patterns of risk.\nYou will be able to answer key questions about the relevant spatial scale of analysis for a given problem (e.g. neighborhoods vs. cities) and identify key spatial statistics for characterizing spatial clustering relevant to your public health topics of interest.  At the end of this term, you will have a strong foundation for pursuing additional study of specific topics in spatial epidemiology, advanced statistical methods relevant to spatial analysis, and the creation of maps and other data visualizations that effectively convey these ideas."
  },
  {
    "objectID": "epid684/index.html#laying-the-foundations-what-is-spatial-epidemiology",
    "href": "epid684/index.html#laying-the-foundations-what-is-spatial-epidemiology",
    "title": "Zelnotes",
    "section": "Laying the Foundations: What is spatial epidemiology?",
    "text": "Laying the Foundations: What is spatial epidemiology?\nIn this first module, we will start to get our heads around what we actually mean when we talk about spatial epidemiology. First and foremost, we’ll ask the question of why space matters in public health, when it does and does not, and what tools are available to us to diagnose and address spatial problems where they exist.\n\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\n01/05 (Thu)\nWhere are we going?\n\n\n01/10 (Tue)\nMaps and More\n\n\n01/12 (Thu)\nSpatial epidemiology is all about relationships\n\n\n01/17 (Tue)\nWhat tools can we use to understand spatial variation and clustering?"
  },
  {
    "objectID": "epid684/index.html#spatial-analysis-and-the-foundations-of-modern-epidemiology",
    "href": "epid684/index.html#spatial-analysis-and-the-foundations-of-modern-epidemiology",
    "title": "Zelnotes",
    "section": "Spatial analysis and the foundations of modern epidemiology",
    "text": "Spatial analysis and the foundations of modern epidemiology\nIn this module, we will use an important but often-mischaracterized historical example - the 1854 Broad Street Cholera outbreak and John Snow’s historic analysis and intervention in it - to explore what it is that makes a public health problem a spatial one. Specifically, we will look at how this particular outbreak was shaped by the confluence of biological, social, and environmental mechanisms.\nAnother important component of this story is one of problem definition: Is the problem transmission of an infectious pathogen, or the spatial aggregation of foul-smelling, disease-causing air?\nDuring this module, we will use Snow’s challenges in convincing a skeptical public and medical establishment of the infectious etiology of Cholera as a springboard for the type of problem definition that is the focus of the Postcards from the Road assignment. This will culminate in presentations at the end of this module, and we will use some of the class time prior to these presentations to make progress on this project.\n\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\n01/19 (Thu)\nWalking into a Cholera epidemic\n\n\n01/24 (Tue)\nClass cancelled\n\n\n01/26 (Thu)\nMiasma vs. Germ Theory\n\n\n\nWhat is the origin? assignment due\n\n\n01/31 (Tue)\nUsing maps to make the case\n\n\n02/02 (Thu)\nFrom the pump handle to the present"
  },
  {
    "objectID": "epid684/index.html#introducing-the-multi-level-approach-to-spatial-variation",
    "href": "epid684/index.html#introducing-the-multi-level-approach-to-spatial-variation",
    "title": "Zelnotes",
    "section": "Introducing the multi-level approach to spatial variation",
    "text": "Introducing the multi-level approach to spatial variation\n\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\n02/07 (Tue)\nDistinguishing between individual and place effects\n\n\n02/09 (Thu)\nIntroduction to hierarchical models of spatial variation"
  },
  {
    "objectID": "epid684/index.html#destination-project-progress",
    "href": "epid684/index.html#destination-project-progress",
    "title": "Zelnotes",
    "section": "Destination Project Progress",
    "text": "Destination Project Progress\n\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\n02/14 (Tue)\nDestination presentations\n\n\n02/16 (Thu)\nIn-class project workshop"
  },
  {
    "objectID": "epid684/index.html#clustering",
    "href": "epid684/index.html#clustering",
    "title": "Zelnotes",
    "section": "Clustering!",
    "text": "Clustering!\n\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\n02/21 (Tue)\nA step-by-step work-through of the radon example\n\n\n02/23 (Thu)\nCancelled due to 🥶 Ice Storm 🥶\n\n\n\nSpring break - no class! 02/28 (Tue) & 03/02 (Thu)\n\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\n03/07 (Tue)\nMaking sense of clustered environmental exposures\n\n\n03/09 (Thu)\nClustered environmental exposures cont’d"
  },
  {
    "objectID": "epid684/index.html#postcards-from-the-road",
    "href": "epid684/index.html#postcards-from-the-road",
    "title": "Zelnotes",
    "section": "Postcards from the Road",
    "text": "Postcards from the Road\n\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\n03/14 (Tue)\nPostcards from the Road I\n\n\n03/16 (Thu)\nPostcards from the Road II\n\n\n03/21 (Tue)\nPostcards from the Road III\n\n\n03/23 (Thu)\nPostcards from the Road IV"
  },
  {
    "objectID": "epid684/index.html#connecting-measures-of-risk-clustering-to-spatial-variation-in-health-outcomes",
    "href": "epid684/index.html#connecting-measures-of-risk-clustering-to-spatial-variation-in-health-outcomes",
    "title": "Zelnotes",
    "section": "Connecting measures of risk clustering to spatial variation in health outcomes",
    "text": "Connecting measures of risk clustering to spatial variation in health outcomes\n\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\n03/28 (Tue)\nConnecting residential segregation to inequitable disease outcomes\n\n\n03/30 (Thu)\nProject Workshop\n\n\n04/04 (Tue)\nLocal variation in segregation exposure and its impact on individual health outcomes\n\n\n04/06 (Thu)\nSegregation, structural racism, and infection"
  },
  {
    "objectID": "epid684/index.html#coming-around-the-bend-the-present-and-future-of-spatial-epidemiology",
    "href": "epid684/index.html#coming-around-the-bend-the-present-and-future-of-spatial-epidemiology",
    "title": "Zelnotes",
    "section": "Coming around the bend: The present and future of spatial epidemiology",
    "text": "Coming around the bend: The present and future of spatial epidemiology\n\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\n04/11 (Tue)\nFinal Product Workshop\n\n\n04/13 (Thu)\nConnecting local variation in vaccination behavior to population-level outbreak risks\n\n\n04/18 (Tue)\nLast Day! Wrap-Up\n\n\n04/26 (Wed)\nFinal Product Submission"
  },
  {
    "objectID": "epid684/local_clustering_vpd_risk.html",
    "href": "epid684/local_clustering_vpd_risk.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Please read (Masters et al. 2021), which an analysis of the impact of a change in vaccine exemption rules on the local spatial clustering of non-medical exemptions to vaccination in Michigan.\n\n\n\n\nThe reading for today’s class utlizes the local Moran’s I statistic, sometimes also referred to as a LISA statistic. During class, in addition to discussing the reading, we will complete this interactive tutorial about the calculation and interpretation of LISA statistics and local Moran’s I in particular."
  },
  {
    "objectID": "epid684/local_segregation_disease_risk.html",
    "href": "epid684/local_segregation_disease_risk.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Please read (Kershaw et al. 2017), which is an analysis of longitudinal data looking specifically at the impact of changes in exposure to residential segregation on the cardiovascular health of Black Americans in several U.S. cities.\n\n\n\n\nWe will discuss the reading by Kershaw et al. (2017) as well as revisiting Biello et al. (2012) and Woo et al. (2021) from a few class sessions ago.\n\n\n\nSlides\nMiro board for in-class activity"
  },
  {
    "objectID": "epid684/meetings.html",
    "href": "epid684/meetings.html",
    "title": "Course Meetings",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nThursday, January 5, 2023\n\n\nFirst Day: Introduction\n\n\n\n\nTuesday, January 10, 2023\n\n\nMaps and More\n\n\n\n\nThursday, January 12, 2023\n\n\nSpatial Epidemiology is All About Relationships\n\n\n\n\nTuesday, January 17, 2023\n\n\nTools for making sense of spatial variation\n\n\n\n\nTuesday, January 17, 2023\n\n\nWalking into a Cholera Epidemic\n\n\n\n\nThursday, January 26, 2023\n\n\nWhat is the origin?\n\n\n\n\nMonday, February 20, 2023\n\n\nWhat is the destination?\n\n\n\n\nMonday, March 20, 2023\n\n\nPostcards from the road\n\n\n\n\nWednesday, April 26, 2023\n\n\nFinal Product\n\n\n\n\n\n\nMiasmas and (spatial) mental models\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "epid684/policies.html",
    "href": "epid684/policies.html",
    "title": "Zelnotes",
    "section": "",
    "text": "I believe strongly that this course is a collaborative endeavor, and that the primary objective of this class is to help you clarify and achieve your own academic and professional goals. Our course policies are developed with that in mind and with the goal of creating an environment of mutual respect that also fosters intellectual exploration, creativity and productive risk-taking. If at any point you feel that these goals are not being met, please do not hesitate to reach out to me directly.\n\n\nThis class will be held in-person and students are asked and expected to attend in-person whenever possible. If you do need to miss class for whatever reason, please let me know ahead of time so that we can plan accordingly.\nI will record each class session using Zoom and make the link to these recordings available to the entire class. When we have a guest speaker, these sessions will be recorded if the visiting speaker consents.\nIf you are unable to attend in person but can join via Zoom, that will be an option. However, since the class is heavily focused on discussion, in-class activities, and project work, the utility of these recordings and remote participation will be limited primarily to listening to and reviewing the lecture portions of the class.\n\n\n\nDue dates for assignments have been chosen to ensure that there is enough time to give detailed feedback prior to the due date for the next assignment. However, I know that life often intervenes in the form of either personal obligations or other assignments which can make it difficult to get every assignment in on time. If you do not anticipate being able to make a due date, please let me know as far ahead of time as possible so that we can arrange an extension.\n\n\n\nI believe that detailed, qualitative feedback from instructors and peers is far more useful than numerical grading. Numerical grading schemes disincentivize exploration and risk-taking, rewarding consistency over the course of a term over growth. Assignment assessments and final grades for this course are constructed with the goal of facilitating growth and exploration within a student-centered environment. This, of course, entails a slightly different set of expectations and responsibilities for students and instructors than in a course following a classic grading/assessment scheme.\n\n\n\nThere will be no numerical or letter grades given for written assignments or presentations, including the final product of your semester-long project. Instead, you will receive written feedback from me on each assignment, and will also have a number of opportunities to share your ideas and receive constructive feedback from your classmates.\nThis will include in-class presentations on your research topic, structured peer-reviews of work-in-progress, in-class ‘workshop’ time to work with your project group and get advice from me and your classmates, as well as periodic self-assessments that you will submit to me.\nFollowing fall break, I will ask you to submit responses to a midterm check-in survey, which will allow me to better understand what is working and not working for you in the class so that we can make adjustments for the second half of the term.\n\n[!NOTE] For peer and self-assessments, I will provide a template containing an outline of the areas I am asking you to give feedback on. These assessment opportunities are meant to help keep you on track and to flag individual and collective issues (i.e. unclear assignment instructions, questions about assignment topics, format, etc. as they arise).\n\n\n\n\nFinal course grades will be self-assigned and based on each student’s assessment of their progress towards their self-identified goals as well as those identified by me over the course of the term. I reserve the right to revise final grades upwards or downwards if necessary, but will only do this only in rare cases where a student’s self-assessed grade is strongly out of alignment with my perception of their performance in the course, and in all others will take the student’s self-assessed grade as-is.\n\n\n\n\n\n\nExceptions to self-grading\n\n\n\nThe primary factors that would result in a downward revision of a student’s self-assessed grade are 1) chronic, unexplained lateness in turning in assignments or not completing assignments, 2) instances of academic dishonesty.\n\n\nWhile consistent in-person attendance is expected, I understand that personal, professional and academic circumstances may periodically impact your ability to attend either in-person or remotely. I will not take attendance during regular course sessions and an excused absence is not required on most days.\n\n\n\n\n\n\nIn-class presentations and feedback\n\n\n\nHowever, on days when students are presenting and giving/receiving feedback, attendance or an excused absence is required and unexplained/excused absence may result in a downward revision of your final grade. This requirement reflects a recognition that on these days of class, individual attendance impacts all course members.\n\n\nStudents are also welcome to take the course under the Satisfactory/Unsatisfactory (S/U) grading scheme offered by SPH if desired."
  },
  {
    "objectID": "epid684/postcards_presentations.html",
    "href": "epid684/postcards_presentations.html",
    "title": "Zelnotes",
    "section": "",
    "text": "JZ Slides\n\n\nLaybohr Kamara I, Wang L, Guo Y, et al. Spatial–temporal heterogeneity and determinants of HIV prevalence in the Mano River Union countries. Infectious Diseases of Poverty. 2022;11(1):116. doi:10.1186/s40249-022-01036-1\n\n\n\nClarke P, George LK. The Role of the Built Environment in the Disablement Process. Am J Public Health. 2005;95(11):1933-1939. doi:10.2105/AJPH.2004.054494\n\n\n\n\n\n\nStopka, T. J., Amaravadi, H., Kaplan, A. R., Hoh, R., Bernson, D., Chui, K. K. H., Land, T., Walley, A. Y., LaRochelle, M. R., & Rose, A. J. (2019). Opioid overdose deaths and potentially inappropriate opioid prescribing practices (PIP): A Spatial Epidemiological Study. International Journal of Drug Policy, 68, 37–45. https://doi.org/10.1016/j.drugpo.2019.03.024\n\n\n\nFinlay J, Esposito M, Langa KM, Judd S, Clarke P. Cognability: An Ecological Theory of neighborhoods and cognitive aging. Social Science & Medicine. 2022;309:115220. doi:10.1016/j.socscimed.2022.115220\n\n\n\n\n\n\nFralick M, Nott C, Moggridge J, et al. Detection of Covid-19 Outbreaks Using Built Environment Testing for SARS-CoV-2. NEJM Evidence. Published online February 17, 2023. doi:10.1056/EVIDoa2200203\nSlides\n\n\n\nKaufman, E. J., Morrison, C. N., Branas, C. C., & Wiebe, D. J. (2018). State firearm laws and interstate firearm deaths from homicide and suicide in the United States: a cross-sectional analysis of data by county. JAMA Internal Medicine, 178(5), 692-700. doi:10.1001/jamainternmed.2018.0190\n\n\n\n\n\n\nBharti N, Tatem AJ, Ferrari MJ, Grais RF, Djibo A, Grenfell BT. Explaining Seasonal Fluctuations of Measles in Niger Using Nighttime Lights Imagery. Science. 2011;334(6061):1424-1427. doi:10.1126/science.1210554\n\n\n\nBrooks MM, Siegel SD, Curriero FC. Characterizing the spatial relationship between smoking status and tobacco retail exposure: Implications for policy development and evaluation. Health & Place. 2021;68:102530. doi:10.1016/j.healthplace.2021.102530"
  },
  {
    "objectID": "epid684/roadmap-project/assignment-destination.html",
    "href": "epid684/roadmap-project/assignment-destination.html",
    "title": "Zelnotes",
    "section": "",
    "text": "What is the destination?\nWith your problem clearly defined in your origin assignment, the next step is to describe in concrete terms what it would look like if your problem was solved or at least meaningfully addressed. This could be reducing incidence of a particular disease in a community, increasing knowledge of risk and protective factors among at-risk individuals, or closing a gap in scientific knowledge of the problem, or something else you identify as being important.\nIn other words, I am asking you to identify a concrete target for improvement in your problem. This could be quantitative, e.g. a 10% reduction in risk of death from cardiovascular disease in a given community, or something more qualitative. For example, if your identified problem is around a knowledge or data gap rather than reducing disease incidence, it could be focused on developing a scientific study or evaluation project to understand the mechanisms driving your outcome. Please think broadly about what the nature of the improvement you wish to make is and connect that to the format of your final project.\nThis assignment will take the form of an in-class ‘lightning’ presentation of 8-10m in mid-Feburary. in your presentation, please briefly:\n\nOutline the problem and argue for its importance,\nPresent your target 🎯 for improvement based on your research and understanding of what has already been tried in this area.\nPropose a format for your final product that will facilitate making progress towards your identified target. This could be a research paper, a technical report, a blog post, a data visualization, or anything else you think would advance your goal and allow you to develop or showcase your skills.\n\nBecause these presentations are necessarily short, we will have limited time for real-time Q&A during this session, but we will use a google doc to keep track of comments and suggestions in real time during the course session. This will give you useful feedback that will help guide your project going forward."
  },
  {
    "objectID": "epid684/roadmap-project/assignment-final.html",
    "href": "epid684/roadmap-project/assignment-final.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Final Product\nFor your culminating assignment, I am asking you to take a step along the path linking your origin problem to your proposed destination.\nThis could be any or the following:\n\nA proposal for foundation or government funding to implement the approach you have selected in an affected community,\nAn op-ed or blog post making the case for your approach to the public or policymakers,\na policy position paper, or a grant proposal for research funds.\n\nYou could also implement a component of your proposed approach by:\n\nConducting a spatial data analysis\nConstructing a visually-appealing report or pamphlet to be shared with community members,\nRecording and editing a podcast episode making the case for your approach, making a set of maps, an interactive data visualization,\nOr something else that works for your problem.\n\nIt is OK for the final product to be a work-in-progress at the end of the term. I would far prefer to see you be ambitious and start something you can continue to carry forward than to narrow your ambitions to meet an end-of-term deadline."
  },
  {
    "objectID": "epid684/roadmap-project/assignment-origin.html",
    "href": "epid684/roadmap-project/assignment-origin.html",
    "title": "Zelnotes",
    "section": "",
    "text": "What is the origin?\nFor this assignment, I’m asking you to put together an overview of your spatial problem of interest, where you:\n\nDescribe the biological, social and environmental factors that are known or suspected to drive spatial variation in your outcome.\nHighlight the populations and geographic areas most impacted by it.\nDescribe and argue for the public health importance of a spatial approach to addressing this problem.\n\nIn this assignment, you will begin building the foundation that will eventually lead to your [[Final Product Submission]] assignment. The purpose of this assignment is for you to build a useful reference for yourself that can inform work on your project going forward. Over the course of the term, you will build this reference to form a coherent plan for your final product.\nThink of this as being more like a well-annotated lab notebook or a portfolio than a formal research paper. It should still be well-documented and clearly written, but can include different sources of content including references to academic papers, news articles, video clips, and anything else you think is relevant or important for understanding your topic.\n\nDefining your problem\nYou should define your problem with as much specificity as possible. For example, if you are interested in Tuberculosis (TB), rather than discussing spatial variation of TB infection at a very high level, focus on the problem of TB in a specific city or region. Making things concrete will be key for keeping your project focused and making it useful and interesting to you."
  },
  {
    "objectID": "epid684/roadmap-project/assignment-postcard.html",
    "href": "epid684/roadmap-project/assignment-postcard.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Postcards from the road\nIn this assignment, I am asking you to take over the class for part of a session (~40m) and use this time to:\n\nShare your expertise on your project topic with the class.\nShowcase the progress you have made on your project to this point.\nGet critical feedback on this progress from me and your peers as we head into the final stretch of the semester.\n\nIn preparation for your turn leading part of a class session, I am asking you to:\n\nAssign a single reading relevant to your project topic, which other members of the class will read beforehand and come to class prepared to discuss. Prior to settling on a reading, I will ask you to send me three candidate papers and I will help you select one that is well-suited to educating the rest of the class about your project area and stimulating discussion.\nPrepare to lead discussion and/or an in-class activity relevant to your topic area and the assigned reading. This should involve a mini-lecture (5-10m) providing additional background on your topic area that aids in the interpretation of the reading and prompts for discussion. If you would like to run an in-class activity of some kind that goes beyond posing questions to the class as a whole, please feel free to do so! This could involve small groups, writing a short reflection, etc.\nProvide a brief update on your project progress (5-10m), identifying both your ongoing successes and sticking points/frustrations which you could use constructive feedback on soliving."
  },
  {
    "objectID": "epid684/roadmap-project/index.html",
    "href": "epid684/roadmap-project/index.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Roadmap Project\nThroughout the term, you will complete a project in which you identify a problem of interest to you in spatial epidemiology and plot a path towards solving it. You will build and receive feedback on this project from me and your peers over the course of the term. In keeping with the spatial theme of the course, we will call this the Roadmap project because you will be asked to chart your own path from a spatial problem to a solution to the problem you have identified.\nThe nature of the problem and solution are up to you, but we will all approach it with the objective of being able to apply all of the following skills by the time you complete the project.\nAlthough a lot of the work on this project will be done outside of our course meetings, we will devote several class sessions to working on and presenting materials developed through completing the steps of this project. The stages of this project will address all of the course goals, with a particular focus on the fourth one - focused on integrating different sources of information on a spatial problem in epidemiology - via the final product of this semester-long project.\n\nProject Stages\nYou will build towards the final project in stages over the course of the term through the following four sub-assignments:\n\nWhat is the origin?\nWhat is the destination?\nPostcard from the road\nFinal Product\n\n\n\nAssessment and Grading\nYou will be evaluating your own progress and the match between what you were able to accomplish and your goals at the outset. The point of having this be self-assessed is to make sure you pick something that is of interest and use to you, not what you imagine what I want from you. Please spend some time thinking about how this project could best address your short- and long-term academic and career goals.\nThis could be anything from learning a new analytical skill, deepening your knowledge about a particular epidemiological topic or set of policy/practice tools.\nI encourage you to think about other uses for the final product of this semester-long project: How could you use it to lay the foundation for a fellowship application, create a portfolio to showcase your work to potential employers, or do anything else that is important to you?\nFor example, your final product could include any of the following:\n\nA podcast episode introducing your topic and some of your ideas/research on it.\nA spatial data visualization with accompanying explanation; can be presented in the form of a set of social media posts, a blog entry, or as a printed document.\nA policy white paper meant to inform public and political debate around issues related to your problem of choice.\nA pamphlet or some other type of public health communication meant to be shared in practice setting, e.g. in a clinic, during outreach, etc.\n\nIt could also very well be something not on this list - you just need to justify why you are taking the approach you choose."
  },
  {
    "objectID": "epid684/segregation_infection.html",
    "href": "epid684/segregation_infection.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Adkins-Jackson PB, Chantarat T, Bailey ZD, Ponce NA. Measuring Structural Racism: A Guide for Epidemiologists and Other Health Researchers. American Journal of Epidemiology. Published online September 25, 2021:kwab239. doi:10.1093/aje/kwab239\nAcevedo-Garcia D. Residential segregation and the epidemiology of infectious diseases. Social Science & Medicine. 2000;51(8):1143-1161. doi:10.1016/S0277-9536(00)00016-2\n\n\n\n\nIn addition to discussing the readings, we will work through a hands-on activity where we can explore the ideas in Acevedo-Garcia (2000) using an infectious disease transmission model.\n\n\n\nSlides"
  },
  {
    "objectID": "epid684/segregation_social_clustering.html",
    "href": "epid684/segregation_social_clustering.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Today’s class will be held on Zoom due to illness.\n\n\n\n\nBefore class, please read:\nWoo H, Brigham EP, Allbright K, et al. Racial Segregation and Respiratory Outcomes among Urban Black Residents with and at Risk of Chronic Obstructive Pulmonary Disease. Am J Respir Crit Care Med. 2021;204(5):536-545. doi:10.1164/rccm.202009-3721OC 📄\nBiello KB, Kershaw T, Nelson R, Hogben M, Ickovics J, Niccolai L. Racial Residential Segregation and Rates of Gonorrhea in the United States, 2003–2007. Am J Public Health. 2012;102(7):1370-1377. doi:10.2105/AJPH.2011.300516 📄\n\n\nThese are useful references on the segregation measures employed in the assigned papers:\nMassey DS, Denton NA. The Dimensions of Residential Segregation. Social Forces. 1988;67(2):281-315. doi:10.2307/2579183\nU.S. Census Bureau reference on segregation measures\n\n\n\n\n\nWe will discuss the assigned readings and complete a hands-on activity focused on better understanding the similarities and differences between different population-level measures of spatial segregation & clustering.\n\n\n\nSlides"
  },
  {
    "objectID": "epid684/session_10_hierarchical_models.html",
    "href": "epid684/session_10_hierarchical_models.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Read Gelman (2006) 📄, which is a worked example of a multi-level model of variation in household radon intensity, which is an important environmental exposure\n\n\n\nNotes on notation used for describing spatial statistical models\n\n\n\n\n\nDiscussion of the Gelman paper and hierarchical regression models more generally.\nHands-on with some simulated multi-level data and models.\n\n\n\n\n\nSlides\n\n\n\nThree approaches to analyzing clustered data"
  },
  {
    "objectID": "epid684/session_6_miasma.html",
    "href": "epid684/session_6_miasma.html",
    "title": "Zelnotes",
    "section": "",
    "text": "In this session, we will discuss the competing causal explanations for infection during the London Cholera Outbreak, specifically miasma theory, which posited that infection was transmitted solely via noxious air, and germ theory, which posited that there was an infectious agent transmitted from person to person. One of the big takeaways from this session’s readings is that Snow used spatial information to make the argument that observed spatial patterns of Cholera could not be explained well at all by miasma theory, but instead was more consistent with the transmission of some infectious agent, i.e. V. Cholerae. \n\n\nContinue reading The Ghost Map, Ch 3-4 (pp 56-109) (Johnson 2007)\n\n\n\nWe will begin to explore the concept of a natural experiment, which is what Snow’s experimenta crucis (described in the readings) was an early example of. We will discuss both how such studies can be valuable, and also how they can go wrong, or at least open up a number of questions while answering others.\n\n\n\nSlides\nSpatial density estimation exercise"
  },
  {
    "objectID": "epid684/session_7_using_maps_to_make_the_case.html",
    "href": "epid684/session_7_using_maps_to_make_the_case.html",
    "title": "Zelnotes",
    "section": "",
    "text": "In this session, we will finally make our way into Golden Square and its ongoing outbreak, and see how the things Snow had learned from his prior investigations came together in the context of the outbreak unfolding in front of his eyes. We will finish our reading and discussion of The Ghost Map and complete a hands-on activity in which we can apply some more modern tools of spatial analysis to the Golden Square data.\n\n\n\nContinue reading The Ghost Map, Chs 6 & 7, pp 139-190.\nChapter 5 of The Ghost Map is not required, but may provide helpful additional context if you have time to read it.\n\n\n\n\n\nHands-on with the Snow Data\n\n\n\n\n\nSlides\n\nZoom recording"
  },
  {
    "objectID": "epid684/session_9_between_place_individual_variation.html",
    "href": "epid684/session_9_between_place_individual_variation.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Please read (Chaix, Merlo, and Chauvin 2005) 📄 and (Merlo et al. 2005) 📄.\n\n\nIn class, we will spend time discussing the concept of intraclass correlation and the conceptual links between this measure and the other ideas of clustering we have explored so far.\n\n\n\n\nSlides\nZoom recording of lecture\n\n\nNotes on notation used for describing spatial statistical models\nMeasuring clustering with the Intraclass Correlation Coefficient"
  },
  {
    "objectID": "epid684/session_destination_workshop.html",
    "href": "epid684/session_destination_workshop.html",
    "title": "Zelnotes",
    "section": "",
    "text": "The goal of today’s class session is to digest feedback from the lightning presentations on tuesday and to use this feedback to make a plan for your project going forward over the rest of the term.\n\n\n\n\n\n\nOur goal is to finish class today with a clear and concise proposal for your final project.\n\n\n\nWe are going to use this session to help solidify your overarching goals for your project and to translate that into a concrete proposal about the focus and format of your final product.\n\n\n\n\n\nOn your own: Review comments and feedback from your presentation. (~20m)\n\nAddress and respond to comments in the document about areas of confusion and suggestions for your final project.\nHas this feedback and discussion changed your thinking about what your final product should be? If so, how?\nIn this document, write up a 1-2 paragraph proposal for your final product, including a short, descriptive title. Include information on:\n\nWhat public health outcomes would you like to impact with your project? The project is likely just one step along a longer path to that impact, but it keeping these goals in mind will help you be specific about what you want to and to orient towards achieving those longer-term goals. Don’t be bashful about being ambitious and expansive here!\nWho is the ideal audience for your work? Is this information most useful for the general public? For a specialist scientific audience? For working public health professionals? Policymakers? Someone else? All of the above?\nWhat format do you think will work best for your work? This could be a policy brief, a research paper, a data visualization, a blog post, a podcast episode, or anything else that matches the goals of the project with the audience you identified above. Think about this in terms of both public health and personal/professional interest: Is there a particular format that will help you either acheive an academic goal or showcase your skills and experience to practice partners/employers?\nWhat is feasible this term? Given the time we have left, what should the scope of the project be for the rest of the term? Do you envision continuing this work after the semester ends? How does that impact your planning?\n\n\nPitch your proposal and get peer feedback (~40m)\n\nIn groups of four, each member will briefly summarize their response to the original peer feedback and briefly explain and advocate for their final project idea to the group.\nGroup members will read each pitch and provide comments in the document during the discussion.\n\nRevise your proposal (~20m)\n\nBased on the feedback from your small-group discussions, make any revisions you think are necessary to your final proposal.\nDuring this time, I will also come around and check in with you individually.\n\n\n\n\n\nFeedback document"
  },
  {
    "objectID": "epid684/session_environmental_exposure.html",
    "href": "epid684/session_environmental_exposure.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Before class, please read:\nJerrett M, Burnett RT, Ma R, et al. Spatial Analysis of Air Pollution and Mortality in Los Angeles Epidemiology. 2005;16(6):727-736. doi:10.1097/01.ede.0000181630.15826.7d 📄\n\n\n\n\nIn class, we will discuss the Jerrett paper and continue exploring the Minnesota radon data from a more explicitly spatial perspective. You can find the tutorial we will work through in detail here.\n\n\n\nSlides"
  },
  {
    "objectID": "epid684/session_radon_walkthrough.html",
    "href": "epid684/session_radon_walkthrough.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Lash TL. The Harm Done to Reproducibility by the Culture of Null Hypothesis Significance Testing. American Journal of Epidemiology. 2017;186(6):627-635. doi:10.1093/aje/kwx261\n\n\n\nIn class, we will be doing a walk-through of how to reproduce the analysis in the radon paper we reviewed in a previous class session. To be able to run this, please come to class with your computer set up to run the example:\n\nInstall or update to the latest version of RStudio. The tutorial code will be contained in a Quarto markdown document. Quarto is an updated version of the venerable RMarkdown, and the newest versions of RStudio include Quarto support by default.\nSet up your R/RStudio installation to be able to load the following packages using the following code:\n\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(bayesplot)\nlibrary(rstanarm)\nlibrary(purrr)\nlibrary(tidybayes)\n\nIf you are not sure if you have these installed or want to update to the latest versions, please paste this command into a running R session to download and install:\n\ninstall.packages(c(\"ggplot2\",\"tidyr\",\"dplyr\",\"bayesplot\",\"rstanarm\",\"purrr\",\"tidybayes\"))\n\nIf you run into problems with any of these, please let me know!\n\n\n\n\n\nSlides\nYou can find the tutorial we are going to work through here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zelnotes",
    "section": "",
    "text": "Welcome to my personal blog and teaching website. My name is Jon Zelner, and I am an Associate Professor of Epidemiology at the University of Michigan School of Public Health (UM SPH), where I conduct research and teach about the social and spatial epidemiology of infectious disease. I am also a core faculty member of the Center for Social Epidemiology and Population Health, also at UM SPH.\nI lead the EpiBayes research group, which is also focused on these issues, as well as the development of Bayesian statistical methods for applied epidemiology."
  },
  {
    "objectID": "index.html#teaching",
    "href": "index.html#teaching",
    "title": "Zelnotes",
    "section": "Teaching",
    "text": "Teaching\nFor more information about courses I teach, please see the homepage for the following classes at the University of Michigan:\n\nEpidemiology 684: Spatial Epidemiology"
  },
  {
    "objectID": "index.html#blog-tutorials",
    "href": "index.html#blog-tutorials",
    "title": "Zelnotes",
    "section": "Blog & Tutorials",
    "text": "Blog & Tutorials\nFor blog entries and assorted tutorials on epidemiological methods and spatial analysis, please check out my blog."
  },
  {
    "objectID": "posts/backwards-design/index.html",
    "href": "posts/backwards-design/index.html",
    "title": "What I learned from teaching online.",
    "section": "",
    "text": "Me, teaching online\n\n\nRepost of a post I put up on the Statistical Modeling, Causal Inference, and Social Science blog\nhttps://statmodeling.stat.columbia.edu/2022/01/06/what-i-learned-from-teaching-online/\n\n\n\n\n\nCitationBibTeX citation:@online{zelner2022,\n  author = {Jon Zelner},\n  title = {What {I} Learned from Teaching Online.},\n  date = {2022-01-06},\n  url = {https://zelnotes.io/posts/backwards-design},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJon Zelner. 2022. “What I Learned from Teaching Online.”\nJanuary 6, 2022. https://zelnotes.io/posts/backwards-design."
  },
  {
    "objectID": "posts/csr/index.html#set-up-the-environment",
    "href": "posts/csr/index.html#set-up-the-environment",
    "title": "Coming to grips with complete spatial randomness",
    "section": "Set up the environment",
    "text": "Set up the environment\n\nknitr::opts_chunk$set(message = FALSE, warning=FALSE, tidy=TRUE)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(spdep)"
  },
  {
    "objectID": "posts/csr/index.html#calculating-morans-i",
    "href": "posts/csr/index.html#calculating-morans-i",
    "title": "Coming to grips with complete spatial randomness",
    "section": "Calculating Moran’s I",
    "text": "Calculating Moran’s I\nThe function below will calculate Moran’s I for a given grid. For more information on this function, please see this tutorial where it is described in more detail:\n\nmoranFromSF <- function(x, sfdf, style = \"B\") {\n    nb <- poly2nb(sfdf)\n    lw <- nb2listw(nb, style = style, zero.policy = TRUE)\n    mi <- moran(x, lw, length(nb), Szero(lw), NAOK = TRUE)$I\n    return(mi)\n}"
  },
  {
    "objectID": "posts/csr/index.html#plotting",
    "href": "posts/csr/index.html#plotting",
    "title": "Coming to grips with complete spatial randomness",
    "section": "Plotting",
    "text": "Plotting\nThe code below uses the function we just defined to aggregate the data we generated above to a 5 x 5 grid, plot the proportion cases vs. controls in each grid cell, and label the plot with the estimated value of Moran’s I:\n\npg <- pointToGrid(df, n = 5)\ndata_mi <- round(moranFromSF(pg$p, pg), 2)\ng <- ggplot(pg) + geom_sf(aes(fill = p)) + scale_fill_viridis_c() + ggtitle(paste0(\"Moran's I=\",\n    data_mi))\nplot(g)\n\n\n\n\n\n\n\n\n\n\nChange the number of cells\n\n\n\nThe parameter n in the function above controls the number of cells in each direction, so n=5 will result in \\(n^2 = 25\\) cells, n=10 will result in 100 cells, etc. Try different values of n and see how they impact the visual pattern of the grid as well as the value of Moran’s I. Note that grayed out cells are those with NA values, i.e. ones where there are no points (cases or controls) present."
  },
  {
    "objectID": "posts/csr/index.html#permuting-the-data",
    "href": "posts/csr/index.html#permuting-the-data",
    "title": "Coming to grips with complete spatial randomness",
    "section": "Permuting the data",
    "text": "Permuting the data\nThe function below does just that - it takes the input data and shuffles the values of the case/control outcome so that they no longer have a spatial pattern:\n\nrandomizeOutcome <- function(df) {\n    new_df <- df\n    new_df$z <- sample(df$z, size = nrow(df), replace = FALSE)\n    return(new_df)\n}\n\nWhen we plot the randomized data, we can see now that the strong clustering pattern at the beginning (see Figure 1) has been disrupted but that the proportion of individuals that are cases remains the same:\n\nrandom_df <- randomizeOutcome(df)\ng <- ggplot(random_df, aes(x = x, y = y)) + geom_point(aes(colour = z)) + facet_wrap(~z) +\n    ggtitle(paste0(\"Pr(z=1)=\", round(sum(random_df$z)/nrow(random_df), 2)))\n\nplot(g)\n\n\n\n\nAnd we can use the functions we defined earlier to aggregate up from the randomized data to grid cells, calculate Moran’s I, and plot:\n\npg <- pointToGrid(random_df)\nmi <- round(moranFromSF(pg$p, pg), 2)\ng <- ggplot(pg) + geom_sf(aes(fill = p)) + scale_fill_viridis_c() + ggtitle(paste0(\"Moran's I=\",\n    mi, \", Pr(z=1)=\", round(sum(random_df$z)/nrow(random_df), 2)))\nplot(g)"
  },
  {
    "objectID": "posts/csr/index.html#approximating-the-sampling-distribution-of-spatially-random-data",
    "href": "posts/csr/index.html#approximating-the-sampling-distribution-of-spatially-random-data",
    "title": "Coming to grips with complete spatial randomness",
    "section": "Approximating the sampling distribution of spatially random data",
    "text": "Approximating the sampling distribution of spatially random data\nHere’s where the rubber meets the road 🛣. We will repeat the randomization above a large number of times (careful, as the number of trials increases, so does the runtime), save the value of Moran’s I for each randomized dataset, and then compare the randomized values to the ‘true’ one."
  },
  {
    "objectID": "posts/csr/index.html#generate-some-data-1",
    "href": "posts/csr/index.html#generate-some-data-1",
    "title": "Coming to grips with complete spatial randomness",
    "section": "Generate Some Data",
    "text": "Generate Some Data\nFirst, we’ll generate and visualize the structured data again:\n\ndf <- clusteredOutcomeData(0.01, 20, 20, 100)\npg <- pointToGrid(df)\ndata_mi <- round(moranFromSF(pg$p, pg), 2)\ng <- ggplot(pg) + geom_sf(aes(fill = p)) + scale_fill_viridis_c() + ggtitle(paste0(\"Moran's I=\",\n    data_mi, \", Pr(z=1)=\", round(sum(df$z)/nrow(df), 2)))\nplot(g)\n\n\n\n\nNow we’ll generate a large number of datasets which have the same number of cases and controls and where the points are all in the same locations, but the geographic distribution of case and control statuses is randomly distributed:\n\nn_trials <- 1000\nmi_vals <- rep(0, n_trials)\ng <- st_as_sf(df, coords = c(\"x\", \"y\"))\na <- st_make_grid(g, what = \"polygons\", n = 5) %>%\n    st_sf\na$cell <- 1:nrow(a)\npp <- st_join(g, a, st_intersects) %>%\n    as_tibble %>%\n    select(z, cell)\n\nnb <- poly2nb(a)\nlw <- nb2listw(nb, style = \"B\", zero.policy = TRUE)\n\nfor (i in 1:n_trials) {\n    random_df <- randomizeOutcome(pp) %>%\n        group_by(cell) %>%\n        summarize(p = sum(z)/n())\n    tmp_pg <- left_join(a, random_df)\n    mi_vals[i] <- moran(tmp_pg$p, lw, length(nb), Szero(lw), NAOK = TRUE)$I\n}"
  },
  {
    "objectID": "posts/csr/index.html#comparing-randomized-data-to-observed-data",
    "href": "posts/csr/index.html#comparing-randomized-data-to-observed-data",
    "title": "Coming to grips with complete spatial randomness",
    "section": "Comparing randomized data to observed data",
    "text": "Comparing randomized data to observed data\nLet’s re-generate the clustered data from before and then compare it to some randomized values:\n\ndata_mi <- round(moranFromSF(pg$p, pg), 2)\n\np <- sum(mi_vals >= data_mi)/length(mi_vals)\ng <- ggplot() + geom_histogram(aes(x = mi_vals), binwidth = 0.025) + xlab(\"Moran's I value\") +\n    geom_vline(xintercept = data_mi, colour = \"red\") + geom_vline(xintercept = median(mi_vals),\n    colour = \"green\") + ggtitle(paste0(\"Randomized values of Moran's I vs. observed (pr <=\",\n    p, \")\"))\n\nplot(g)\n\n\n\n\n\n\n\n\n\n\nTry patterns with weak or noisy clustering\n\n\n\nModify the values of or_x and or_y to make the clustering in the simulated data weaker (i.e. make these paramters smaller). You can also reduce the total number of points (n) to see how easy it is to distinguish between ‘true’ clustering and random noise using a smaller dataset."
  },
  {
    "objectID": "posts/d3-sir/index.html",
    "href": "posts/d3-sir/index.html",
    "title": "Building a simple SIR model with javascript and d3",
    "section": "",
    "text": "Another throwback from my old blog about constructing a simple SIR model in javascript and d3.\n\nhttps://jzelner.github.io/2015/02/12/sir/\n\n\n\n\n\nCitationBibTeX citation:@online{zelner2023,\n  author = {Jon Zelner},\n  title = {Building a Simple {SIR} Model with Javascript and D3},\n  date = {2023-01-20},\n  url = {https://zelnotes.io/posts/d3-sir},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJon Zelner. 2023. “Building a Simple SIR Model with Javascript and\nD3.” January 20, 2023. https://zelnotes.io/posts/d3-sir."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "A crash course on modeling infection inequities\n\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nJon Zelner\n\n\n\n\n\n\n  \n\n\n\n\nPostdoctoral or research staff position modeling social inequity in infectious disease dynamics at the University of Michigan\n\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nJon Zelner\n\n\n\n\n\n\n  \n\n\n\n\nComing to grips with complete spatial randomness\n\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nJon Zelner\n\n\n\n\n\n\n  \n\n\n\n\nTaking a spatial perspective on the radon data\n\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nJon Zelner\n\n\n\n\n\n\n  \n\n\n\n\nRe-creating the radon teaching example with rstanarm and tidybayes\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nJon Zelner\n\n\n\n\n\n\n  \n\n\n\n\nMaking sense of spatial density estimation\n\n\n\n\n\n\n\ncode\n\n\ntutorial\n\n\nstatistics\n\n\nspatial\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nJon Zelner\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a simple SIR model with javascript and d3\n\n\n\n\n\n\n\ntutorial\n\n\nmapping\n\n\nd3\n\n\njavascript\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nJon Zelner\n\n\n\n\n\n\n  \n\n\n\n\nVoronoi intensity mapping with d3\n\n\n\n\n\n\n\ntutorial\n\n\nmapping\n\n\nd3\n\n\njavascript\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nJon Zelner\n\n\n\n\n\n\n  \n\n\n\n\nCodifying Tobler’s First Law using Locally Weighted Regression\n\n\n\n\n\n\n\ncode\n\n\ntutorial\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nJon Zelner\n\n\n\n\n\n\n  \n\n\n\n\nWhat I learned from teaching online.\n\n\nOr: How I learned to stop worrying and just plan my teaching and writing.\n\n\n\n\n\n\n\n\n\nJan 6, 2022\n\n\nJon Zelner\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/infection-inequity-postdoc/index.html",
    "href": "posts/infection-inequity-postdoc/index.html",
    "title": "Postdoctoral or research staff position modeling social inequity in infectious disease dynamics at the University of Michigan",
    "section": "",
    "text": "The EpiBayes research group, led by Dr. Jon Zelner in the Dept. of Epidemiology and Center for Social Epidemiology and Population Health (CSEPH) at the University of Michigan School of Public Health, seeks a postdoctoral fellow or research staff member to work with us on a new, interdisciplinary, NIH-funded project focused on developing new ways of modeling infectious disease dynamics which account for the joint social and biological drivers of infection inequality.\nPlease contact Jon Zelner (jzelner@umich.edu) with any inquiries related to this position. If you ae interested in applying, please include a brief CV and a short description of your research interests and experiences."
  },
  {
    "objectID": "posts/infection-inequity-postdoc/index.html#topic-areas",
    "href": "posts/infection-inequity-postdoc/index.html#topic-areas",
    "title": "Postdoctoral or research staff position modeling social inequity in infectious disease dynamics at the University of Michigan",
    "section": "Topic Areas",
    "text": "Topic Areas\nPotential projects include (but are not limited to):\n\nDevelopment of theoretical models of the social dynamics of infectious disease transmission.\nSpatial statistical models fit to complex spatiotemporal datasets, and other opportunities.\nProjects will have a primary focus on respiratory infections including SARS-CoV-2 and seasonal and pandemic influenza, but other infectious disease systems characterized by significant socioeconomic inequality are also potential foci."
  },
  {
    "objectID": "posts/infection-inequity-postdoc/index.html#research-and-mentorship-team",
    "href": "posts/infection-inequity-postdoc/index.html#research-and-mentorship-team",
    "title": "Postdoctoral or research staff position modeling social inequity in infectious disease dynamics at the University of Michigan",
    "section": "Research and Mentorship Team",
    "text": "Research and Mentorship Team\nThe individual who fills this position will work directly with Dr. Zelner as well as key collaborators on the project at the University of Michigan School of Public Health including Dr. Marisa Eisenberg, Dr. Emily Martin, and Dr. Abram Wagner. This individual will also work with project partners Dr. Nigel Paneth at Michigan State University, Dr. Sanyu Mojola at Princeton University and Dr. Merlin Chowkwanyun at Columbia University’s Mailman School of Public Health.\nPlease contact Jon Zelner (jzelner@umich.edu) with any inquiries related to this position."
  },
  {
    "objectID": "posts/infection-inequity-postdoc/index.html#responsibilities",
    "href": "posts/infection-inequity-postdoc/index.html#responsibilities",
    "title": "Postdoctoral or research staff position modeling social inequity in infectious disease dynamics at the University of Michigan",
    "section": "Responsibilities",
    "text": "Responsibilities\n\nConduct collaborative research on the social determinants of inequity in infectious disease risks.\nDevelop novel quantitative models of infectious disease transmission that account for structural drivers of infection inequity.\nPublication of peer-reviewed manuscripts.\nEngagement with government and community stakeholders and partners.\nPresentation of research at local, national and international conferences and meetings."
  },
  {
    "objectID": "posts/radon/index.html#preparing-to-run-the-examples",
    "href": "posts/radon/index.html#preparing-to-run-the-examples",
    "title": "Re-creating the radon teaching example with rstanarm and tidybayes",
    "section": "Preparing to run the examples",
    "text": "Preparing to run the examples\nTo be able to run the code below locally, please do the following:\n\nInstall or update to the latest version of RStudio. The tutorial code will be contained in a Quarto markdown document. Quarto (which powers this very website!) is an updated version of the venerable RMarkdown, and the newest versions of RStudio include Quarto support by default.\nSet up your R/RStudio installation to be able to load the following packages using the following code:\n\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(arm)\nlibrary(bayesplot)\nlibrary(rstanarm)\nlibrary(purrr)\nlibrary(tidybayes)\n\nIf you are not sure if you have these installed or want to update to the latest versions, please paste this command into a running R session to download and install:\n\ninstall.packages(c(\"ggplot2\",\"tidyr\",\"dplyr\",\"bayesplot\",\"rstanarm\",\"purrr\",\"tidybayes\"))\n\n3.Download the zipfile containing this tutorial, unzip it and open an R session inside this newly unzipped radon directory."
  },
  {
    "objectID": "posts/radon/index.html#setting-up-the-workspace",
    "href": "posts/radon/index.html#setting-up-the-workspace",
    "title": "Re-creating the radon teaching example with rstanarm and tidybayes",
    "section": "Setting up the workspace",
    "text": "Setting up the workspace\nFirst, we will load the relevant packages:\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(bayesplot)\nlibrary(rstanarm)\nlibrary(purrr)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/radon/index.html#data-preparation",
    "href": "posts/radon/index.html#data-preparation",
    "title": "Re-creating the radon teaching example with rstanarm and tidybayes",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, lets take the raw radon dataset from the rstanarm package and recode the floor variable to be interpretable as the basement one from the original paper: some minor modifications and additonal datasets that we’ll use for the purposes of modeling and visualizing these data.\n\nradon$basement <- 1 - radon$floor\n\nNow we can see that that the dataset has all of the variables we need:\n\n\n  floor county  log_radon log_uranium basement\n1     1 AITKIN 0.83290912  -0.6890476        0\n2     0 AITKIN 0.83290912  -0.6890476        1\n3     0 AITKIN 1.09861229  -0.6890476        1\n4     0 AITKIN 0.09531018  -0.6890476        1\n5     0  ANOKA 1.16315081  -0.8473129        1\n6     0  ANOKA 0.95551145  -0.8473129        1"
  },
  {
    "objectID": "posts/radon/index.html#door-1-full-pooling",
    "href": "posts/radon/index.html#door-1-full-pooling",
    "title": "Re-creating the radon teaching example with rstanarm and tidybayes",
    "section": "🚪 Door 1: Full pooling!",
    "text": "🚪 Door 1: Full pooling!\nThis corresponds to a model in which we are assuming exactly no variation across locations in terms of the baseline level of radon. So, we can run a simple regression model where we assume that:\n\\[\ny_{ij} = \\alpha + \\beta x_{ij} + \\epsilon_{i}\n\\]\nWhere \\(x_{ij} = 1\\) if a house has a basement and 0 otherwise.\nIn R, we can fit this model via least squares using a single line of code:\n\nm1 <-lm(log_radon ~ basement, data = radon)\n\nWe can call the summary function to get a description of the key coefficients and the goodness-of-fit:\n\n\nlm(formula = log_radon ~ basement, data = radon)\n            coef.est coef.se\n(Intercept) 0.78     0.06   \nbasement    0.59     0.07   \n---\nn = 919, k = 2\nresidual sd = 0.79, R-Squared = 0.07"
  },
  {
    "objectID": "posts/radon/index.html#door-2-no-pooling",
    "href": "posts/radon/index.html#door-2-no-pooling",
    "title": "Re-creating the radon teaching example with rstanarm and tidybayes",
    "section": "🚪 Door 2: No pooling",
    "text": "🚪 Door 2: No pooling\nThe second approach is the “No Pooling” one in which we allow the baseline intensity of radon in each county (represented by the intercept term \\(\\alpha_j\\)) to vary, but we don’t do anything to constrain that variation. In other words, we treat each county as though it was independent.\nHowever, to estimate a consistent effect of having a basement across all counties, we estimate a single \\(\\beta\\) term. This leads to a model that looks like this:\n\\[\ny_{ij} = \\alpha_j + \\beta x_{ij} + \\epsilon_{i}\n\\]\nIn R this is easy to implement, because we are implicitly asking the regression model to treat county as a categorical variable if we pass it to it as a factor datatype:\n\nno_pool_m <- lm(log_radon ~ basement + log_uranium + county, data = radon)\n\n\n\nlm(formula = log_radon ~ basement + log_uranium + county, data = radon)\n                     coef.est coef.se\n(Intercept)           0.42     0.37  \nbasement              0.69     0.07  \nlog_uranium           0.32     0.60  \ncountyANOKA           0.09     0.44  \ncountyBECKER          0.48     0.53  \ncountyBELTRAMI        0.67     0.43  \ncountyBENTON          0.39     0.48  \ncountyBIGSTONE        0.31     0.68  \ncountyBLUEEARTH       0.83     0.51  \ncountyBROWN           0.80     0.60  \ncountyCARLTON         0.05     0.38  \ncountyCARVER          0.43     0.50  \ncountyCASS            0.52     0.47  \ncountyCHIPPEWA        0.56     0.60  \ncountyCHISAGO         0.21     0.48  \ncountyCLAY            0.79     0.54  \ncountyCLEARWATER      0.28     0.50  \ncountyCOOK           -0.23     0.60  \ncountyCOTTONWOOD      0.06     0.63  \ncountyCROWWING        0.26     0.40  \ncountyDAKOTA          0.27     0.36  \ncountyDODGE           0.63     0.63  \ncountyDOUGLAS         0.60     0.49  \ncountyFARIBAULT      -0.42     0.57  \ncountyFILLMORE        0.18     0.75  \ncountyFREEBORN        0.94     0.51  \ncountyGOODHUE         0.80     0.48  \ncountyHENNEPIN        0.32     0.34  \ncountyHOUSTON         0.52     0.66  \ncountyHUBBARD         0.30     0.44  \ncountyISANTI          0.22     0.57  \ncountyITASCA          0.07     0.42  \ncountyJACKSON         0.83     0.59  \ncountyKANABEC         0.18     0.50  \ncountyKANDIYOHI       0.94     0.54  \ncountyKITTSON         0.51     0.55  \ncountyKOOCHICHING     0.04     0.52  \ncountyLACQUIPARLE     1.75     0.71  \ncountyLAKE           -0.40     0.44  \ncountyLAKEOFTHEWOODS  0.99     0.51  \ncountyLESUEUR         0.60     0.55  \ncountyLINCOLN         1.08     0.67  \ncountyLYON            0.75     0.59  \ncountyMAHNOMEN        0.23     0.84  \ncountyMARSHALL        0.53     0.44  \ncountyMARTIN         -0.05     0.51  \ncountyMCLEOD          0.17     0.46  \ncountyMEEKER          0.13     0.49  \ncountyMILLELACS      -0.07     0.60  \ncountyMORRISON        0.11     0.41  \ncountyMOWER           0.54     0.51  \ncountyMURRAY          1.27     0.90  \ncountyNICOLLET        0.99     0.59  \ncountyNOBLES          0.71     0.68  \ncountyNORMAN          0.10     0.63  \ncountyOLMSTED         0.16     0.48  \ncountyOTTERTAIL       0.60     0.40  \ncountyPENNINGTON      0.11     0.54  \ncountyPINE           -0.24     0.43  \ncountyPIPESTONE       0.62     0.68  \ncountyPOLK            0.55     0.60  \ncountyPOPE            0.11     0.70  \ncountyRAMSEY          0.22     0.33  \ncountyREDWOOD         0.78     0.61  \ncountyRENVILLE        0.46     0.67  \ncountyRICE            0.70     0.49  \ncountyROCK            0.06     0.79  \ncountyROSEAU          0.64     0.36  \ncountySCOTT           0.70     0.43  \ncountySHERBURNE       0.24     0.44  \ncountySIBLEY          0.10     0.58  \ncountySTLOUIS        -0.03     0.31  \ncountySTEARNS         0.38     0.43  \ncountySTEELE          0.41     0.53  \ncountySTEVENS         0.56     0.77  \ncountySWIFT          -0.18     0.61  \ncountyTODD            0.65     0.54  \ncountyTRAVERSE        0.76     0.69  \ncountyWABASHA         0.69     0.50  \ncountyWADENA          0.43     0.48  \ncountyWASECA         -0.47     0.58  \ncountyWASHINGTON      0.31     0.34  \ncountyWATONWAN        1.54     0.60  \ncountyWILKIN          1.06     0.86  \ncountyWINONA          0.41     0.60  \ncountyWRIGHT          0.59     0.39  \n---\nn = 919, k = 86\nresidual sd = 0.73, R-Squared = 0.29"
  },
  {
    "objectID": "posts/radon/index.html#sec-partial",
    "href": "posts/radon/index.html#sec-partial",
    "title": "Re-creating the radon teaching example with rstanarm and tidybayes",
    "section": "🚪 Door 3: Partial Pooling",
    "text": "🚪 Door 3: Partial Pooling\nFinally, we get to the partial pooling, hierarchical model in which we introduce a hierarchical prior to the model to allow our model to shrink observations from places with few observations towards the population mean. This allows us to avoid the pitfalls of overfitting associated with the no-pooling approach while not making the homogeneity assumptions associated with the full-pooling approach.\nThis works out to a multi-level model that allows random variation in household-level radon measurements as well as variation at the county level in radon levels above or below the amount predicted by the county-level soil uranium measure. Much like the no-pooling model, we can write outcomes for individuals as:\n\\[\ny_{ij} = \\alpha_j + \\beta x_{ij} + \\epsilon_{i}\n\\]\nHowever, rather than stopping there, we introduce a second level of random variation to the county-level intercepts, \\(\\alpha_j\\).\n\\[\n\\alpha_j = \\gamma_0 + \\gamma \\zeta_{j} + \\epsilon_{j}\n\\]\nWhere \\(\\epsilon_i \\sim N(0, \\sigma_i)\\) and \\(\\epsilon_j \\sim N(0, \\sigma_j)\\).\nTo fit this model, we’ll use the rstanarm package, which uses the Stan Bayesian modeling language under the hook to fit the model. This model introduces another piece of syntax to our equation, which now reads log_radon ~ basement + log_uranium + (1 | county). The interesting part of this is the (1 | county) which is a syntax used by rstanarm and other hierarchical modeling packages (such as lme4) to specify random intercepts (typically represented by a 1 in the matrix of regressors) for each of a set of clusters, in this case counties. In this model, the county-level intercept terms are implicitly assumed to be normally distributed with unknown variance \\(\\sigma_j\\) which will be estimated when the model is fit.\nWe use the stan_lmer function to fit a hierarchical linear model with a normally-distributed response variable, as follows:\n\nm2 <- stan_lmer(log_radon ~ basement + log_uranium + (1 | county), data = radon)\n\nBecause this model is fit by MCMC, we can use draws from the posterior distribution to understand uncertainty in the model. For example, this visualization of the median prediction and credible intervals for the basement and uranium effects can be visualized using the mcmc_areas function from the bayesplot package:\n\nposterior <- as.matrix(m2)\ng2 <- mcmc_areas(posterior, pars = c(\"basement\", \"log_uranium\"))\nplot(g2)"
  },
  {
    "objectID": "posts/radon/index.html#figure-1",
    "href": "posts/radon/index.html#figure-1",
    "title": "Re-creating the radon teaching example with rstanarm and tidybayes",
    "section": "Figure 1",
    "text": "Figure 1\n\nData Preparation\nSince each row of radon dataset includes an observation of a single house, we need to work backwards to obtain the county-level soil uranium measure for each individual county. This is pretty straightforward to do using the dplyr package:\n\ncounty_uranium <- radon %>%\n  group_by(county) %>%\n  summarize(log_uranium = first(log_uranium)) \n\nWe will also make a second dataset that we will use for storing the predicted radon levels for households with and without basements each for county. This contains 2 entries for each county, representing observations taken in the basement or on the first floor.\n\ncounty_uranium_tmp_1 <- county_uranium\ncounty_uranium_tmp_1$basement <- 1\ncounty_uranium_tmp_2 <- county_uranium\ncounty_uranium_tmp_2$basement <- 0\n\ncounty_dummy_df <- rbind(county_uranium_tmp_1, county_uranium_tmp_2)\n\nNow, we will take each of our fitted models (fully pooled, unpooled and partially pooled) and put their predicted values into our plotting dataset\n\ncounty_dummy_df$pooled_pred <- predict(m1, county_dummy_df)\ncounty_dummy_df$no_pool_pred <- predict(no_pool_m, county_dummy_df)\n\nWarning in predict.lm(no_pool_m, county_dummy_df): prediction from a rank-\ndeficient fit may be misleading\n\n\nBecause the partial pooling model was fit using MCMC, we will take a slightly different approach and use the median of the posterior predictive distribution for each observation, which is analogous to (but not exactly the same as) the OLS predictions from the other models:\n\n## Gives posterior median for each prediction.\ncounty_dummy_df$partial_pred <- posterior_predict(m2, county_dummy_df) %>%\n  apply(2,median) \n\n\n\nPlotting\nTo re-create Figure 1, we will subset out the observed data and predictions for the 8 counties included in the original figure:\n\n## Place the county names in a vector we will use to keep track of them\nfig_1_counties <-\n  c(\n    \"LACQUIPARLE\",\n    \"AITKIN\",\n    \"KOOCHICHING\",\n    \"DOUGLAS\",\n    \"CLAY\",\n    \"STEARNS\",\n    \"RAMSEY\",\n    \"STLOUIS\"\n  )\n\n\n# First, using the `county_dummy_df` with the basement/non-basement predictions in it,\n# subset out the relevant counties and make a new county factor variable which\n# will be used to ensure that the counties in Fig. 1 plot in the right order\n\ncounty_df_fig_1 <- county_dummy_df %>%\n  filter(county %in% fig_1_counties) %>%\n  mutate(county2 = factor(county, levels = fig_1_counties)) %>%\n  arrange(county)\n\n## Now select out the households in the original data that\n## are in each county and create another county-level factor\n## variable in the same order\n\npred_counties <- radon %>% filter(county %in% fig_1_counties) %>%\n  mutate(county2 = factor(county, levels = fig_1_counties))\n\nOnce we have the datasets together for the figure, we can begin constructing it using ggplot2:\n\ng <- ggplot() +\n  ## The geom_jitter geom plots the log_radon values for each household and \n  ## jitters the points slightly to avoid overplotting. \n  geom_jitter(\n    data = pred_counties,\n    aes(x = basement, y = log_radon, group = county2),\n    height = 0,\n    width = 0.1\n  ) +\n  \n  ## This superimposes the partial-pooling (α + β x_i + ϵ_i +ϵ_j) predictions\n  ## over the raw data\n  geom_line(\n    data = county_df_fig_1,\n    aes(x = basement, y = partial_pred, group = county2),\n    linetype = \"solid\",\n    colour = \"gray\"\n  ) +\n  \n  ## No-pooling predictions (α_{ij} + β x_i + ϵ_i)\n  geom_line(\n    data = county_df_fig_1, \n    aes(x = basement, y = no_pool_pred, group = county2)\n  ) +\n  \n  ## Full pooling predicitons (α + β x_i + ϵ_i)\n  geom_line(\n    data = county_df_fig_1,\n    aes(x = basement, y = pooled_pred, group = county2),\n    linetype = \"dashed\"\n  ) +\n  \n  ## Finally, use facet_wrap to arrange the panels in two \n  ## rows of four\n  facet_wrap(vars(county2), nrow = 2) +\n  xlab(\"basement\") +\n  ylab(\"log radon level\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())\n\nplot(g)"
  },
  {
    "objectID": "posts/radon/index.html#figure-2",
    "href": "posts/radon/index.html#figure-2",
    "title": "Re-creating the radon teaching example with rstanarm and tidybayes",
    "section": "Figure 2",
    "text": "Figure 2\nFigure 2 reproduces the relationship between the county-level random intercepts, \\(\\alpha_j\\) and the expected level of radon at a county level as a function of county-level soil uranium.\n\nData Preparation\nThe following code allows us to extract predictions at the county level using our prediction dataset. To do this, we use the predicted_draws function from the tidybayes package, which lets us sample from the posterior distribution of the fitted model. The median_qi function, also from tidybayes, lets us calculate the width of a 1 standard error interval (equivalent to the range containing ~17% of the posterior probability mass around the posterior median) used in the original Figure 1 from Gelman (2006):\n\ndd <- predicted_draws(m2, county_dummy_df) %>%\n  median_qi(.width = 0.17) %>%\n  filter(basement == 0)\n\nIn order to calculate the predicted mean radon at a county level, we need to access the coefficients corresponding to the level two model, including the intercept \\(\\gamma_0\\) and the effect of a 1-log change in log-uranium on predicted log-radon, \\(\\gamma_1\\). In order to get these values out of the model, we can use the gather_draws function from tidybayes, which allows us to access the posterior distributions for each of these parameters:\n\nuranium_coefs <-\n  gather_draws(m2, c(`(Intercept)`, log_uranium)) %>% median_qi()\n\nNow it is as simple as calculating the linear predictor \\(\\gamma_0 + \\gamma_1 z_j\\), where \\(z_j\\) is the log-uranium measure for the j-th county, and storing this information in a data frame we will use for plotting:\n\nlog_uranium_range <-\n  seq(min(county_uranium$log_uranium) - .1,\n      max(county_uranium$log_uranium) + .1,\n      by = 0.1)\n\npred_log_radon <-\n  uranium_coefs$.value[1] + uranium_coefs$.value[2] * log_uranium_range\n\nmedian_radon_pred <-\n  data.frame(log_uranium = log_uranium_range, .prediction = pred_log_radon)\n\n\n\nPlotting\nNow, we can build this figure up one step at a time, starting with our mean predictions:\n\ng <- ggplot(dd) +\n     geom_line(data = median_radon_pred, aes(x = log_uranium, y = .prediction)) \n\nplot(g)\n\n\n\n\nThe next step is to then add the median predictions (points) and 1 SE errorbars to the plot, and then fix the theme to match the original figure, et voilà!\n\ng <- g +  geom_point(aes(x = log_uranium, y = .prediction, group = county)) +\n  geom_errorbar(aes(\n    x = log_uranium,\n    y = .prediction,\n    ymin = .lower,\n    ymax = .upper\n  )) +\n  theme_bw() + theme(panel.grid.major = element_blank(),\n                     panel.grid.minor = element_blank()) +\n  xlab(\"county-level uranium measure\") +\n  ylab(\"regression intercept\")\n\nplot(g)"
  },
  {
    "objectID": "posts/scenario-modeling-hub-talk/index.html",
    "href": "posts/scenario-modeling-hub-talk/index.html",
    "title": "A crash course on modeling infection inequities",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "posts/scenario-modeling-hub-talk/index.html#a-brief-and-incomplete-annotated-bibliography-of-resources-on-modeling-acute-infection-inequities",
    "href": "posts/scenario-modeling-hub-talk/index.html#a-brief-and-incomplete-annotated-bibliography-of-resources-on-modeling-acute-infection-inequities",
    "title": "A crash course on modeling infection inequities",
    "section": "A brief (and incomplete!) annotated bibliography of resources on modeling acute infection inequities",
    "text": "A brief (and incomplete!) annotated bibliography of resources on modeling acute infection inequities\n\nStructural Racism and Infection\n\n(Adkins-Jackson et al. 2021) provides a very useful guide for thinking about how epidemiological models and modelers should account for the impacts of racism on disease outcomes. This piece has specific recommendations for analysts around model construction and accounting for multiple, intersecting identities.\n(Noppert, Hegde, and Kubale 2022) articulate a framework based on fundamental cause theory for accounting for and addressing inequities in infection risk. (Zelner, Naraharisetti, and Zelner 2023) is an invited commentary that builds on some of the ideas in the original piece.\n(Chowkwanyun and Reed 2020) is an excellent piece from early in the COVID-19 pandemic which highlights many of the challenges and pitfalls associated with a focus on racial health disparities without adequate attention to their material causes.\n\n\n\nModeling for Equity\n\n(Zelner et al. 2022) and (Abuelezam et al. 2023) attempt to articulate frameworks for doing a better job of using epidemic models to capture the dynamics of inequity in infection and disease outcomes.\n(Ma et al. 2021) is an excellent data-driven exercise in modeling the linkages between race/ethnicity, essential work and inequity in disease outcomes.\nThis is a hands-on example we put together that translates the framework in (acevedo-garcia2000?) into a simple dynamic model of the relationship between residential segregation and infection.\n\n\n\nMethodological challenges\n\n(Zelner et al. 2020) is an analysis examining the relative impacts of case-fatality vs. incidence of infection in inequities in COVID-19 mortality in Michigan.\n(Trangucci, Chen, and Zelner 2022) presents a computationally efficient Bayesian method for accounting for non-random missingness of race-ethnicity data without making restrictive assumptions and/or using surname coding.\nThis is a hands-on comparison of different metrics of spatial segregation applied to the same simulated dataset."
  },
  {
    "objectID": "posts/smoothing/index.html",
    "href": "posts/smoothing/index.html",
    "title": "Codifying Tobler’s First Law using Locally Weighted Regression",
    "section": "",
    "text": "In this brief tutorial, we will review some basic ideas about smoothing and start thinking through how we can express these ideas mathematically and in R.\nTobler’s profound - but deceptively simple - first law states that:\n\n“Everything is related to everything else. But near things are more related than distant things.” (Tobler 1970)\n\nHe applied this idea to his development of a dynamic model of urban growth in the Detroit region which assumed that rates of population growth were spatially similar:\n\n\nIn this post, we’re going to start with a simpler problem - change in the value of a function in one dimension - to see how we can translate the concept of distance decay implied by Tobler’s first law (TFL) into a useful model. To begin, we’re going to keep it simple with no noise or observation error and just interpolate some values.\n\n\nIn this example, we are interested in visualizing and predicting the values of a function \\(f(x_i)\\) which outputs values \\(y_i\\), the expected value of the output function.\n\\[\ny_i = f(x_i)\n\\]\nLets start by getting the values of \\(f(x)\\) for every input value \\(x\\). For simplicity, we will assume that \\(f(x)\\) is a sine function and that the values of \\(x\\) go from -1 to +5, allowing us to observe one half cycle of the sine function:\n\nx <- seq(-1, 5, by = 0.1)\ny <- sin(x)\nplot(x,y)\n\n\n\n\nYou can see right away that this simple curve pretty neatly expresses Tobler’s first law: \\(f(x)\\) values of each point are in general more similar to each other for nearby values of \\(x\\). If we want to press this idea into real-world practice, we need a model that can translate TFL into quantitative estimates and qualitative visualizations. There are lots of ways to do this, but we’ll focus in on locally-weighted regression, also known LOWESS.\nThe basic idea of a LOWESS regression is to define a window of size \\(k\\) points around each value one wishes to estimate, and calculate a weighted average of the value of those points, which can then be used as the estimated value \\(\\hat{y_j} \\approx f(x_j)\\). We then run the values of these nearest neighbors through a weight function \\(w(x)\\).\nThese weight functions can take a lot of different forms, but we’ll start simple with a uniform one, i.e. just taking the average of the \\(k\\) nearest neighbors, so that \\(\\hat{y} = sum(z(x_i, k))/k\\), where \\(KNNz\\) is a function returning the \\(y\\) values of the k nearest observations to \\(x_i\\). The value of \\(k\\) is sometimes referred to as the bandwidth of the smoothing function: Larger bandwidths use more data to estimate values at each point, smaller ones use less.\n\n\nUsing the fnn package for R, we can find the indices of the \\(k\\) nearest neighbors of each point we want to make an estimate at:\n\nlibrary(FNN)\nk <- 10\nz <- knn.index(x, k=k)\n\nYou can read the output of this function, below, as indicating the indices (\\(i\\)) of the 10 nearest points to each of the values of \\(x\\).\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    2    3    4    5    6    7    8    9   10    11\n [2,]    1    3    4    5    6    7    8    9   10    11\n [3,]    2    4    1    5    6    7    8    9   10    11\n [4,]    5    3    6    2    1    7    8    9   10    11\n [5,]    6    4    7    3    8    2    1    9   10    11\n [6,]    5    7    4    8    3    9    2   10    1    11\n [7,]    8    6    9    5   10    4   11    3   12     2\n [8,]    7    9   10    6   11    5    4   12    3    13\n [9,]   10    8   11    7   12    6    5   13   14     4\n[10,]    9   11    8   12    7   13   14    6    5    15\n\n\nWe can visualize this by picking a point in the middle of the series and its 10 nearest neighbors and the estimated value of \\(\\hat{y_i}\\) obtained by just taking the average of the k nearest points:\n\nlibrary(ggplot2)\n\n## Plot the original data\ng <- ggplot() + geom_point(aes(x=x, y = y)) + \n  xlab(\"x\") + ylab(\"f(x)\")\n\n## Now, get the index for x = 2\nx_index <- which(x==2)\n\n## Show the range of k nearest neighbors of this point\nknn_low <- min(x[z[x_index, ]])\nknn_high <- max(x[z[x_index, ]])\ny_hat <- mean(y[z[x_index,]], )\n\n## Add errorbars to the figure to show the 10 nearest values with the height of the point indicating the estimated value at y_i, denoted by the red dot\ng <- g + geom_errorbarh(aes(xmin = knn_low, xmax = knn_high, y = y_hat)) + geom_point(aes(x=2,y_hat), colour = \"red\")\n\nplot(g)\n\n\n\n\nNotice that if the knn function is applied at the low end of the series, i.e. to the first value, it will use points to the right of that one instead of to either side:\n\nlibrary(ggplot2)\n\n## Plot the original data\ng <- ggplot() + geom_point(aes(x=x, y = y)) + \n  xlab(\"x\") + ylab(\"f(x)\")\n\n## Use the index for the lowest value\nx_index <- 1\n\n## Show the range of k nearest neighbors of this point\nknn_low <- min(x[z[x_index, ]])\nknn_high <- max(x[z[x_index, ]])\ny_hat <- mean(y[z[x_index,]], )\n\n## Add errorbars to the figure to show the 10 nearest values with the height of the point indicating the estimated value at y_i\ng <- g + geom_errorbarh(aes(xmin = knn_low, xmax = knn_high, y = y_hat)) + geom_point(aes(x=x[x_index],y_hat), colour = \"red\")\n\nplot(g)\n\n\n\n\nNow, lets see what happens if we run our smoother over the whole series and take the average of the 10 nearest points for each and compare them to the observed data:\n\ny_hat <- rep(0, length(x))\nfor (i in 1:length(x) ) {\n  y_hat[i] <- mean(y[z[i,]], )\n}\n\nNow plot the predicted vs. the observed values:\n\ng <- ggplot() + geom_point(aes(x=x, y = y)) + \n  xlab(\"x\") + ylab(\"f(x)\") + geom_line(aes(x=x, y=y_hat), colour = \"red\")\nplot(g)\n\n\n\n\nYou can see this does a pretty good job all the way through, except at the edges. Lets try it again with a smaller window - or bandwidth - of 5 and see what happens. First, we’ll write a function that will give us the predicted value of y at each point given a window of size k and an input value:\n\nknn_est <- function(x, y, k) {\n  z <- knn.index(x, k=k)\n  y_hat <- rep(0, length(x))\n  \nfor (i in 1:length(x) ) {\n  y_hat[i] <- mean(y[z[i,]], )\n}\n\n  df <- data.frame(x=x, y = y, yhat = y_hat)\n  return(df)\n}\n\n\npred_df <- knn_est(x, y, 5)\ng <- ggplot(pred_df) + geom_point(aes(x=x, y=y)) + geom_line(aes(x=x,y=yhat),colour=\"red\")\nplot(g)\n\n\n\n\nThis gets rid of a lot of the weird effects at the edges but introduces some noise into the function. What if we make the window bigger, say 20, to get rid of some of the noise?\n\n\n\n\n\nThis seems to make the edge effects worse, as well as the estimates of the function overall worse.\nWhat happens if we go in the opposite direction and shrink the window down to 2?\n\n\n\n\n\n\n\n\n\nWhy does this appear to be more accurate for these data than \\(k=10\\) and \\(k=5\\)?\nWhat would happen if we added observation noise to the values of \\(y_i\\)? Which one of the smoothers do you think would work better then?\nIs the one best value of \\(k\\) for all datasets? How might you go about picking the best one?\nHow does our uniform weight function express Tobler’s first law? What kind of weight function \\(w(x)\\) might do a better job of capturing the notion of distance decay?"
  },
  {
    "objectID": "posts/spatial-density/index.html",
    "href": "posts/spatial-density/index.html",
    "title": "Making sense of spatial density estimation",
    "section": "",
    "text": "When working with spatial data, our analytic task often falls into either of two rough categories.\nIn this tutorial, we’ll dig into the problem of spatial density estimation in one dimension along a spatial transect. The techniques discussed here form the basis of a number of approaches for cluster or hotspot analysis. For examples of smoothing local values of a continuous variable, check out this tutorial."
  },
  {
    "objectID": "posts/spatial-density/index.html#a-motivating-example",
    "href": "posts/spatial-density/index.html#a-motivating-example",
    "title": "Making sense of spatial density estimation",
    "section": "A motivating example",
    "text": "A motivating example\nA spatial transect is an area of space along a line crossing a landscape. These are often used in ecology and forestry to assess the health of an environment, species diversity and other factors. Using a transect can help simplify the problem of spatial analysis down to one dimension rather than the usual two, while still providing a tremendous amount of useful information.\n\n\n\nExample of an ecological transect from the US National Park Service (source)\n\n\nFor example, (Levy et al. 2014) were interested in characterizing the intensity of exposure to triatomine bugs and other insect vectors of the pathogen T. cruzi, which causes Chagas disease.\n\n\n\nTriatoma (left- and right-hand panels) and T. cruzi (center) (source)\n\n\n\n\n\nIntensity of Triatomine infestation along a 2km transect in Arequipa, Peru (Figure from Levy et al. (2014))\n\n\nImagine we are estimating the density of some unknown insect vector along a 1 kilometer transect with the goal of characterizing the risk of infection with a vector-borne illness."
  },
  {
    "objectID": "posts/spatial-density/index.html#kernel-density-estimation-in-one-dimension",
    "href": "posts/spatial-density/index.html#kernel-density-estimation-in-one-dimension",
    "title": "Making sense of spatial density estimation",
    "section": "Kernel density estimation in one dimension",
    "text": "Kernel density estimation in one dimension\nMuch like in our discussion of kernel smoothing of continuous outcomes, kernel functions play a key role in this setting as well. In this case, imagine that the locations of vectors along our transect have been sampled at random from some unknown function \\(f(x)\\) which takes values from 0 (the beginning of the transect) to 1000m (the end).\nWe can use the Kernel function \\(K(d)\\) to approximate the intensity of the outcome of interest at each observed case location \\(x_i\\). Imagine that our observed data have locations \\(x_1, x_2, \\ldots, x_n\\) and that the distance between our point of interest, \\(x_j\\) and each observed point is \\(d_{ij} = | x_j - x_i |\\).\nFinally, lets include a bandwidth parameter, \\(h\\), which controls the width of the window we will use for smoothing. When we put this all together, we can get an estimate of the density of our outcome of interest at location \\(x_j\\) as follows:\n\\[\n\\hat{f_h}(x_j) = \\frac{1}{n} \\sum_{i=1}^{n} K(\\frac{x_j - x_i}{h})\n\\]\nAs you can see below, we can pick a range of kernel functions, but for the sake of simplicity, in this example, we will focus in on a Gaussian, or normal, kernel, which uses the probability density function of a normal distribution to weight points.\nLets start by sampling locations of observed points along a one dimensional line. To keep things interesting, we’ll use a Gaussian mixture distribution with two components:\n\n\n\nComparison of different kernel functions (source)"
  },
  {
    "objectID": "posts/spatial-density/index.html#worked-example",
    "href": "posts/spatial-density/index.html#worked-example",
    "title": "Making sense of spatial density estimation",
    "section": "Worked example",
    "text": "Worked example\nFirst, lets imagine a scenario in which the risk of observing an insect vector steadily decreases as we walk along our transect. However, along the way there is a hotspot of increased risk beyond what we would expect from the smooth decline before and after that spot. For the purpose of this example, we’ll assume that risk decays exponentially with distance from the origin, but that our hotspot is centered at a point 300 meters into the transect. The code below lets us sample the locations of the points along the transect where 🐜 are observed from two distributions:\n\nAn exponential distribution representing smooth decay from the beginning to the end of the transect, and\nA normal distribution representing a hotspot about 150m in width beginning 300m in\n\n\n\n\nThe figure below shows a histogram of locations sampled from \\(f(x)\\) (vertical bars) overlaid with the true value of \\(f(x)\\) in red:\n\n\nCode\nlibrary(ggplot2)\nd_a <- dexp(1:1000, rate = 1/250) \nd_b <- dnorm(1:1000, mean = 300, sd = 50)\ny <- ((1-p_hot))*d_a + (p_hot*d_b)\n\ndens_df <- data.frame(x = 1:1000, y = y)\nxdf <- data.frame(x=x)\n\n\ng <- ggplot(xdf) + geom_histogram(aes(x=x, y=..density..), bins=100) + \ngeom_line(data=dens_df, aes(x=x,y=y), colour=\"red\") +\nxlim(0, 1000) + ylab(\"Density\") + xlab(\"Distance from transect origin (m)\")\nplot(g)\n\n\n\n\n\nNow, imagine we have another set of finely spaced points along the line, and for each, we want to calculate the weight for each. The function below lets us do that:\n\n\n\nThe figure below shows the true value of our density function \\(f(x)\\) in red, the density of points in the simulated data along the x-axis of the ‘rug plot’, and our smoothed density in black, for a bandwidth of \\(h=10\\):\n\n\nCode\nlibrary(ggplot2)\npred_df <- normal_smoother(x, h = 10)\n\ng <- ggplot() + geom_rug(aes(x=x)) + \ngeom_line(data = pred_df, aes(x=x, y=y)) + \nylab(\"Density\") + geom_line(data = dens_df, aes(x=x,y=y), colour=\"red\") + \nxlim(0, 1000)\ndens_ojs <- dens_df\ndens_ojs$y <- dens_ojs$y*cc\nplot(g)\n\n\n\n\n\n\n\n\nNow, lets see what happens if we try this for different values of \\(h\\):\n\n\nCode\nall_df <- data.frame()\nfor (hv in c(5, 10, 20, 50 ,100, 250)) {\n  pred_df <- normal_smoother(x, h = hv)\n  pred_df$h <- hv\n  all_df <- rbind(all_df, pred_df) \n}\n\n  all_df$h <- as.factor(all_df$h)\n\ng <- ggplot(all_df) + geom_rug(aes(x=x)) + \ngeom_line(data = dens_df, aes(x=x,y=y), colour=\"red\") + \ngeom_line(aes(x=x, y=y)) + \nylab(\"Density\") + \nfacet_wrap(~ h) +\nxlim(0, 1000)\n\nplot(g)\n\n\n\n\n\n\n\nCode\nall_df <- data.frame()\nhvals <- seq(1, 100, by = 2)\ndistvals <- seq(-100, 100, by = 1)\nall_kernvals <- data.frame()\nfor (hv in hvals) {\n  pred_df <- normal_smoother(x, h = hv)\n  pred_df$h <- hv\n  pred_df$smoother <- \"gaussian\"\n  all_df <- rbind(all_df, pred_df) \n  all_kernvals <- rbind(all_kernvals,data.frame(x=distvals, y=kdgaussian(distvals, bw = hv), smoother = \"gaussian\", h = hv))\n  \n  pred_df <- normal_smoother(x, h = hv, kern = kduniform, kernp=kpuniform)\n  pred_df$h <- hv\n  pred_df$smoother <- \"uniform\"\n  all_df <- rbind(all_df, pred_df) \n  all_kernvals <- rbind(all_kernvals,data.frame(x=distvals, y=kduniform(distvals, bw = hv), smoother = \"uniform\", h = hv))\n\n  \n  pred_df <- normal_smoother(x, h = hv, kern = kdtricube, kernp=kptricube)\n  pred_df$h <- hv\n  pred_df$smoother <- \"tricube\"\n  all_df <- rbind(all_df, pred_df) \n    all_kernvals <- rbind(all_kernvals, data.frame(x=distvals, y=kdtricube(distvals, bw = hv), smoother = \"tricube\", h = hv))\n    \n  pred_df <- normal_smoother(x, h = hv, kern = kdtriangular, kernp=kptriangular)\n  pred_df$h <- hv\n  pred_df$smoother <- \"triangular\"\n  all_df <- rbind(all_df, pred_df) \n    all_kernvals <- rbind(all_kernvals, data.frame(x=distvals, y=kdtriangular(distvals, bw = hv), smoother = \"triangular\", h = hv))\n\n}\nall_df$y <- all_df$y * cc"
  },
  {
    "objectID": "posts/spatial-density/index.html#trying-different-bandwidths-and-kernels",
    "href": "posts/spatial-density/index.html#trying-different-bandwidths-and-kernels",
    "title": "Making sense of spatial density estimation",
    "section": "Trying different bandwidths and kernels",
    "text": "Trying different bandwidths and kernels\nYou can adjust the range of the bandwidth here to get a better sense of the relationship between the smoothed curve (black) and true density (red). Adjust the bin width for the histogram of the underlying data to get a sense of the fit of the model to the underlying data.\n\nviewof h = Inputs.range([1, 100], {value: 10, step: 2, label: \"Bandwidth (m)\"})\nviewof bw = Inputs.range([5, 100], {value: 10, step: 5, label: \"Bin width (m)\"})\nviewof kern = Inputs.select([\"gaussian\", \"uniform\", \"tricube\", \"triangular\"], {value: \"gaussian\", label: \"Smoothing kernel\"})\n\nnumbins = Math.floor(1000/bw)\n\ndtrans = transpose(hvals)\nPlot.plot({\ny: {grid: true, \nlabel: \"Density\"},\nx: {\nlabel: \"Distance from transect start (m) →\"\n},\n    marks: [\n    Plot.rectY(transpose(sample),Plot.binX({y: \"count\"}, {x: \"loc\", fill: \"steelblue\", thresholds: numbins})),\n    Plot.lineY(dtrans, {filter: d => (d.h == h) && (d.smoother == kern), curve: \"linear\", x:\"x\",y: d => d.y * bw}),\n    Plot.lineY(transpose(dens), {x:\"x\", y: d => d.y * bw, curve:\"linear\", stroke: \"red\"})\n    ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure below shows the relative amount of weight placed on different points as a function of their distance from the point of interest (0, marked by the vertical red line):\n\nkv = transpose(kernvals).filter(d => d.h == h && d.smoother == kern)\nPlot.plot({\ny: {grid: true, label: \"Relative weight of point as compared to origin\"},\nx: {\nlabel: \"Distance from point of interest (m) ↔ \"\n},\nmarks: [\n//Plot.lineY(kv, {filter: d => (d.smoother == kern), x:\"x\", y: d => d.y*1000}),\nPlot.lineY(kv, Plot.normalizeY({x:\"x\", y: \"y\", basis: \"extent\"})),\nPlot.ruleX([0], {stroke: \"red\"})\n]})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions\n\nWhich of the bandwidth options seems to do the best job in capturing the value of \\(f(x)\\)? Why?\nHow does the choice of kernel impact the smoothing?\nHow do the different kernel functions encode different assumptions about distance decay?\nWhat is the relationship between the histogram of the data and the smoother? What do you see as you change the histogram bin width relative to the smoothing bandwidth?"
  },
  {
    "objectID": "posts/spatial-density/index.html#additional-resources",
    "href": "posts/spatial-density/index.html#additional-resources",
    "title": "Making sense of spatial density estimation",
    "section": "Additional Resources",
    "text": "Additional Resources\nPlease see Matthew Conlen’s excellent interactive KDE tutorial"
  },
  {
    "objectID": "posts/spatial_radon/index.html#getting-started",
    "href": "posts/spatial_radon/index.html#getting-started",
    "title": "Taking a spatial perspective on the radon data",
    "section": "Getting Started",
    "text": "Getting Started\nTo try this tutorial on your own, download and unzip this zipfile and open up your R or RStudio session in the resulting directory.\n\n\n\n\n\n\nLook out 👀 for stretch exercises!\n\n\n\nIf you see a box with a 💡 like this, it’s in an invitation to go a bit further. This could be a conceptual question or a chance to write a bit of code to explore the data or outputs of the analysis a bit more."
  },
  {
    "objectID": "posts/spatial_radon/index.html#learning-goals",
    "href": "posts/spatial_radon/index.html#learning-goals",
    "title": "Taking a spatial perspective on the radon data",
    "section": "Learning Goals",
    "text": "Learning Goals\nThe primary goals of this tutorial are to introduce you to:\n\nMerging of non-spatial health exposure or outcome data with spatial metadata.\nCalculation of important spatial summary statistics, e.g. Moran’s I, from such data.\nSpatial analysis of residuals from aspatial regression models of spatially-referenced data."
  },
  {
    "objectID": "posts/spatial_radon/index.html#download-a-shapefile-for-minnesota",
    "href": "posts/spatial_radon/index.html#download-a-shapefile-for-minnesota",
    "title": "Taking a spatial perspective on the radon data",
    "section": "Download a shapefile for Minnesota",
    "text": "Download a shapefile for Minnesota\nFirst, we need to download a shapefile for the state of Minnesota in which each polygon represents an individual county. Thankfully, in R, this is made easy using the excellent tidycensus package:\n\noptions(tigris_use_cache = TRUE)\n\nminnesota <- get_acs(state = \"MN\", geography = \"county\", variables = \"B19013_001\",\n    geometry = TRUE, year = 2020)\n\nTidycensus gives us the data as an sf dataframe containing a number of fields including population estimates, which we can plot straightforwardly using the plot function supplied by the sf package:\n\nplot(minnesota[\"estimate\"])\n\n\n\n\n\nMerge the spatial data with the radon data\nIn its raw form, this spatial dataset isn’t quite ready to merge with the radon data. If we take a peek at the county names in the shapefile, we can see that they don’t quite match the formatting of the ones in the original data:\n\nhead(sort(minnesota$NAME))\n\n[1] \"Aitkin County, Minnesota\"    \"Anoka County, Minnesota\"    \n[3] \"Becker County, Minnesota\"    \"Beltrami County, Minnesota\" \n[5] \"Benton County, Minnesota\"    \"Big Stone County, Minnesota\"\n\n\nWhereas in the radon data we see:\n\nhead(unique(as.character(radon$county)))\n\n[1] \"AITKIN\"   \"ANOKA\"    \"BECKER\"   \"BELTRAMI\" \"BENTON\"   \"BIGSTONE\"\n\n\nThe big differences here are that the shapefile uses: 1) mixed-case county names and 2) includes the name of the state in each label. To make these match the radon dataset, we can use some tools from the stringr package as well as some base R functions:\n\nminnesota <-\n  minnesota %>% mutate(\n    ## Since all of the original county names have the same substring \" County, Minnesota\"\n    ## we can use the str_remove function to pull them out of all of them\n    county = str_remove(NAME, \" County, Minnesota\") %>% \n      ## Since some of the counties  officially have two-word names (e.g. Big Stone)\n      ## which are collapsed in the radon dataset, we will use this function to remove all spaces:\n      str_replace_all(\" \", \"\") %>% \n      ## A few county names include abbreviations indicated by the presence of a '.' (e.g. St. Louis)\n      ## so we will get rid of that bit of punctuation since it is not in the original data\n      str_replace_all(\"\\\\.\", \"\") %>% \n      ## Finally, convert all the county names to uppercase\n      toupper()\n  )\n\nNow, the county labels should match:\n\nhead(sort(minnesota$county))\n\n[1] \"AITKIN\"   \"ANOKA\"    \"BECKER\"   \"BELTRAMI\" \"BENTON\"   \"BIGSTONE\""
  },
  {
    "objectID": "posts/spatial_radon/index.html#preparing-the-radon-dataset",
    "href": "posts/spatial_radon/index.html#preparing-the-radon-dataset",
    "title": "Taking a spatial perspective on the radon data",
    "section": "Preparing the radon dataset",
    "text": "Preparing the radon dataset\nWe will repeat the steps from the earlier tutorial in order to prepare our data for analysis:\n\nradon <- radon %>%\n    mutate(basement = 1 - floor)\n\ncounty_uranium <- radon %>%\n    group_by(county) %>%\n    summarize(log_uranium = first(log_uranium), mean_radon = mean(log_radon))\n\nBecause the sf dataset returned by tidycensus is a dataframe, we can then easily merge the county-level soil uranium concentrations we derived above into the shapefile. We use the left_join function from dplyr to ensure that all of the counties in the original shapefile are represented in the final dataset, even if a soil uranium measure is unavailable for them in the original data:\n\nminnesota_radon <- left_join(minnesota, county_uranium)\n\nWe can then plot the log-uranium measures on the map and see that, in fact, they are quite spatially correlated. We can also see that there appear to be two counties which are missing soil uranium data in the radon dataset. To have a bit more control over our plots, we’ll switch here to using the geom_sf function of ggplot2, which makes plotting geographies from sf objects easy:\n\ng <- ggplot(minnesota_radon) + geom_sf(aes(fill = log_uranium)) + scale_fill_viridis_c() +\n    ggtitle(\"Soil uranium by MN county\")\nplot(g)"
  },
  {
    "objectID": "posts/spatial_radon/index.html#complete-spatial-randomness",
    "href": "posts/spatial_radon/index.html#complete-spatial-randomness",
    "title": "Taking a spatial perspective on the radon data",
    "section": "Complete Spatial Randomness",
    "text": "Complete Spatial Randomness\nWhat we can do, though, is to generate a bunch of random Minnesotas in which there is no relationship between geographic location and median radon, calculate Moran’s I for each of those, and see how our observed data stack up.\n\ncsrMorans <- function(radon, minnesota, trials = 1000, style = \"B\") {\n    county_uranium <- radon %>%\n        group_by(county) %>%\n        summarize(log_uranium = first(log_uranium), mean_radon = mean(log_radon)) %>%\n        left_join(minnesota, .)\n\n    nb <- poly2nb(minnesota)\n    lw <- nb2listw(nb, style = style, zero.policy = TRUE)\n    mv <- moran(county_uranium$mean_radon, lw, length(nb), Szero(lw), NAOK = TRUE)$I\n\n    moran_vals <- rep(0, trials)\n    for (i in 1:trials) {\n\n        county_uranium_random <- radon %>%\n            mutate(log_radon = sample(log_radon, nrow(.), replace = FALSE)) %>%\n            group_by(county) %>%\n            summarize(log_uranium = first(log_uranium), mean_radon = mean(log_radon))\n\n        random_minnesota <- left_join(minnesota, county_uranium_random)\n        moran_vals[i] <- moran(random_minnesota$mean_radon, lw, length(nb), Szero(lw),\n            NAOK = TRUE)$I\n    }\n\n    return(list(midist = moran_vals, mi = mv))\n}\n\ncsr_dist <- csrMorans(radon, minnesota)\n\nWe can use the distribution of Moran’s I values taken from the randomized datasets to benchmark how likely our observed value is to occur by purely random chance. The figure below shows that this is quite unlikely:\n\ng <- ggplot() + geom_histogram(aes(x = csr_dist$midist), bins = 50) + xlab(\"Moran's I value\") +\n    geom_vline(xintercept = csr_dist$mi, colour = \"red\") + geom_vline(xintercept = median(csr_dist$midist),\n    colour = \"green\") + ggtitle(\"Randomized values of Moran's I vs. observed for median household radon\")\n\nplot(g)\n\n\n\n\nAnd we can directly estimate this probability as follows:\n\nreal_moran <- moranFromSF(minnesota_radon$mean_radon, minnesota_radon)\np_moran <- sum(csr_dist$midist >= csr_dist$mi)/length(csr_dist$midist)\nprint(p_moran)\n\n[1] 0\n\n\nFrom 1000 samples, it appears that none of our random datasets yielded a value of Moran’s I \\(\\ge\\) to the observed value, suggesting that it is unlikely that we would observe this value as a simple function of sampling variability.\n\n\n\n\n\n\nWhat could go wrong?\n\n\n\nBefore you move on, take a minute to think about what some of the potential flaws in our CSR-based approach to assessing the meaningfulness or signficance of this result might be."
  },
  {
    "objectID": "posts/spatial_radon/index.html#full-pooling-model",
    "href": "posts/spatial_radon/index.html#full-pooling-model",
    "title": "Taking a spatial perspective on the radon data",
    "section": "Full-pooling model",
    "text": "Full-pooling model\nThe full-pooling model has the following form, in which the variable \\(x_{ij}\\) indicates whether house \\(i\\) in county \\(j\\) has a basement (1) or not (0).\n\\[\ny_{ij} = \\alpha + \\beta x_{ij} + \\epsilon_{i}\n\\]\n\nfull_pooling_model <- lm(log_radon ~ basement, data = radon)\nradon$full_pooling_resid <- resid(full_pooling_model)\nradon$full_pooling_pred <- predict(full_pooling_model)\n\n\n\n\n\n\n\nStoring Model Predictions\n\n\n\nNote that we are storing the residuals and predictions for this model (and the ones below) as a column inside the radon dataframe."
  },
  {
    "objectID": "posts/spatial_radon/index.html#no-pooling",
    "href": "posts/spatial_radon/index.html#no-pooling",
    "title": "Taking a spatial perspective on the radon data",
    "section": "No Pooling",
    "text": "No Pooling\nThe no-pooling model assumes essentially that each county is indepenedent, and includes a categorical variable for the county that the observed household is in:\n\\[\ny_{ij} = \\alpha_j + \\beta x_{ij} + \\epsilon_{i}\n\\]\n\nno_pooling_model <- lm(log_radon ~ basement + county, data = radon)\nradon$no_pooling_resid <- resid(no_pooling_model)\nradon$no_pooling_pred <- predict(no_pooling_model)"
  },
  {
    "objectID": "posts/spatial_radon/index.html#partial-pooling-model",
    "href": "posts/spatial_radon/index.html#partial-pooling-model",
    "title": "Taking a spatial perspective on the radon data",
    "section": "Partial pooling model",
    "text": "Partial pooling model\nThe partial-pooling model is the multi-level analogue to the no-pooling model. For more detail, please see the partial pooling section of the original tutorial.\n\npartial_pool_model <- stan_lmer(log_radon ~ basement + log_uranium + (1 | county),\n    data = radon)\nradon$partial_pooling_resid <- resid(partial_pool_model)\nradon$partial_pooling_pred <- posterior_predict(partial_pool_model) %>%\n    apply(2, mean)"
  },
  {
    "objectID": "posts/spatial_radon/index.html#full-pooling-residuals",
    "href": "posts/spatial_radon/index.html#full-pooling-residuals",
    "title": "Taking a spatial perspective on the radon data",
    "section": "Full Pooling Residuals",
    "text": "Full Pooling Residuals\nWhen we look at the results of the full-pooling model, the residuals that still look pretty spatially clustered, and this is reflected in the value of Moran’s I > 0:\n\nmi <- round(moranFromSF(results_by_county$full_pooling_resid, results_by_county),\n    2)\ng <- ggplot(results_by_county) + geom_sf(aes(fill = full_pooling_resid)) + scale_fill_viridis_c() +\n    ggtitle(paste0(\"Full pooling residuals with I=\", mi))\nplot(g)\n\n\n\n\nThis is probably intuitive: the full pooling model didn’t include any county-level information, so it might not account for all of the sptial variation. On the flipside, if we look at the predictions of the model - reflecting the expected household levels of radon in each county - should we should expect to find that they are spatially un-clustered or also clustered?\n\nmi <- round(moranFromSF(results_by_county$full_pooling_pred, results_by_county),\n    2)\ng <- ggplot(results_by_county) + geom_sf(aes(fill = full_pooling_pred)) + scale_fill_viridis_c() +\n    ggtitle(paste0(\"Full pooling predictions with I=\", mi))\nplot(g)\n\n\n\n\nWait - what? The predictions are also quite clustered, although the pattern looks a bit like a photographic negative of the residual map. It looks like our model is predicting lower values in the northwest corner of the state relative to the rest of the state. How is this possible, if our model doesn’t include contextual information?\nThis might be explained by differences in composition at the county level: maybe houses in some counties are more likely to have basements than in others? If this is the case, then those high-basement counties may have higher avg. levels of radon. So, lets just check and see if our one predictor - the presence or absence of a basement - exhibits any spatial variability?\n\nmi <- round(moranFromSF(results_by_county$p_basement, results_by_county), 2)\ng <- ggplot(results_by_county) + geom_sf(aes(fill = p_basement)) + scale_fill_viridis_c() +\n    ggtitle(paste0(\"Proportion of surveyed households with a basement, I=\", mi))\nplot(g)\n\n\n\n\nWhoops…that looks familiar! It seems like the pattern of spatial variation in the presence/absence of basements may be driving the clustering in our predictions and - by consequence - our residuals!\n\n\n\n\n\n\nSpatially correlated predictors → Spatially correlated predictions\n\n\n\nSometimes, it is easy to forget that the input data may be as or more correlated than the outcome data. In this example, the presence or absence of a basement in a house seems to have a spatial pattern and this impacts the spatial patterning of our predictions and model residuals!\n\n\nSo it looks like we are over-predicting risk in some areas where more surveyed households have basements and under-predicting it in other places where fewer households have basements."
  },
  {
    "objectID": "posts/spatial_radon/index.html#no-pooling-1",
    "href": "posts/spatial_radon/index.html#no-pooling-1",
    "title": "Taking a spatial perspective on the radon data",
    "section": "No Pooling",
    "text": "No Pooling\nOk, so lets try this again with our no-pooling model which at least includes the counties as categorical covariates. Unless something weird is going on, this model should do a good job of explaining spatial variation:\n\n\nCode\nmi <- round(moranFromSF(results_by_county$no_pooling_resid, results_by_county), 2)\ng <- ggplot(results_by_county) + geom_sf(aes(fill = no_pooling_resid)) + scale_fill_viridis_c() +\n    ggtitle(paste0(\"No pooling residuals with I=\", mi))\nplot(g)\n\n\n\n\n\nWell, that’s a bit better, although it does such a good job at explaining away the overall variability in our measurments, we might be concerned that it is overfitting the model through the inclusion of the county level random effects. This is evidenced in the tiny size of the residuals and their minimal variation:\n\n\nCode\ng <- ggplot() + geom_histogram(aes(x = results_by_county$no_pooling_resid))\nplot(g)\n\n\n\n\n\nUnsurprisingly, this model does an excellent job of predicting the spatial patterns in the original data:\n\n\nCode\nmi <- round(moranFromSF(results_by_county$no_pooling_pred, results_by_county), 2)\ng <- ggplot(results_by_county) + geom_sf(aes(fill = no_pooling_pred)) + scale_fill_viridis_c() +\n    ggtitle(paste0(\"No pooling predictions with I=\", mi))\nplot(g)\n\n\n\n\n\nI love this model! It’s perfect! It captures almost the exact same clustering and spatial patterning of risk as the original data.\n\n\n\n\n\n\nDanger!\n\n\n\nWhat is problematic about this model? What limits its usefulness for both interpretation and prediction?"
  },
  {
    "objectID": "posts/spatial_radon/index.html#partial-pooling",
    "href": "posts/spatial_radon/index.html#partial-pooling",
    "title": "Taking a spatial perspective on the radon data",
    "section": "Partial Pooling",
    "text": "Partial Pooling\nWhen we look at the predictions of the partial pooling model, they are notably smoother and more clustered than those of the full- and no-pooling models:\n\n\nCode\nmi <- round(moranFromSF(results_by_county$partial_pooling_pred, results_by_county),\n    2)\ng <- ggplot(results_by_county) + geom_sf(aes(fill = partial_pooling_pred)) + scale_fill_viridis_c() +\n    ggtitle(paste0(\"Partial pooling predictions with I=\", mi))\nplot(g)\n\n\n\n\n\nIf we compare this pattern and intensity of clustering to the log-uranium data, it is clear that the smoothness in the model predictions reflects the relative smoothness and clustering of the soil uranium data:\n\n\nCode\nmi <- round(moranFromSF(minnesota_radon$log_uranium, minnesota_radon), 2)\ng <- ggplot(minnesota_radon) + geom_sf(aes(fill = log_uranium)) + scale_fill_viridis_c() +\n    ggtitle(paste0(\"Soil uranium by MN county (I = \", mi, \")\"))\nplot(g)\n\n\n\n\n\nWhen we look at the residuals, they are still quite un-clustered - similar to the no pooling model, but their magnitude is larger, suggesting that the multi-level model is less suceptible to overfitting:\n\n\nCode\nmi <- round(moranFromSF(results_by_county$partial_pooling_resid, results_by_county),\n    2)\ng <- ggplot(results_by_county) + geom_sf(aes(fill = partial_pooling_resid)) + scale_fill_viridis_c() +\n    ggtitle(paste0(\"Partial pooling residuals with I=\", mi))\nplot(g)\n\n\n\n\n\nBy contrast, the aggregated residuals at the county level are less indicative of overfitting than the no-pooling model, but are still a bit fat-tailed, suggesting that some counties may still be over- or under-fit.\n\n\nCode\ng <- ggplot() + geom_histogram(aes(x = results_by_county$partial_pooling_resid))\nplot(g)"
  },
  {
    "objectID": "posts/voronoi-mapping/index.html",
    "href": "posts/voronoi-mapping/index.html",
    "title": "Voronoi intensity mapping with d3",
    "section": "",
    "text": "This is an oldie but a goody from my old blog.\nYou can find Pt 1 here:\nVoronoi intensity mapping with D3, Part 1\nAnd the sequel is here:\nVoronoi intensity mapping with D3, Part 2\n\n\n\n\n\nCitationBibTeX citation:@online{zelner2023,\n  author = {Jon Zelner},\n  title = {Voronoi Intensity Mapping with D3},\n  date = {2023-01-20},\n  url = {https://zelnotes.io/posts/voronoi-mapping},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJon Zelner. 2023. “Voronoi Intensity Mapping with D3.”\nJanuary 20, 2023. https://zelnotes.io/posts/voronoi-mapping."
  }
]