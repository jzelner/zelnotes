---
author:
- Jon Zelner
header-includes:
- \usepackage{float}
- \usepackage{wrapfig}
- \usepackage{graphicx}
- \usepackage{appendixnumberbeamer}
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
title: Models for clustered data 
institute: EPID 601 \newline University of Michigan School of Public Health  \newline \newline `jzelner@umich.edu` \newline `epibayes.io`
fontsize: 12pt 
output: 
  beamer_presentation:
    theme: "metropolis"
    latex_engine: xelatex
    citation_package: biblatex 
date: \today
bibliography: /Users/jzelner/repos/bibtex-library/jz_library.bib
---

```{r setup, echo=FALSE,warning=FALSE,message=FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_root(".git/index"))
## This function uses the `here` package to make it easy to include graphics in a directory
## relative to the project root
include_local_graphics <- function(x) {
  knitr::include_graphics(here::here(x))
}
```

## Roadmap

- How can we model clustered data? What are the pitfalls of different approaches?

\pause

- Smoothing is at the heart of modeling clustered data: \alert{Hands-on}

\pause

- Break!

\pause

- How quickly can you break a regression model? A hierarchical modeling \alert{hands-on}.

## It's that time of year.

```{r echo=FALSE, out.width='50%', fig.align="center", fig.cap="That November feeling..."}
include_local_graphics("images/sleepy_bee.jpg")
```

## Exposure clustering is a fundamental epidemiological problem

On your own:

 - Where do we find clustering in real-world data?

\pause

In small groups:

- What \alert{mechanisms} induce clustering?

\pause

- How do you think \alert{ignoring} clustering impact the conclusions we draw from data?

\pause

- What are some implications of clustering in the context of \alert{applied epidemiology}?


## Types of clustered data

- Repeated measures of \alert{individuals}, 

\pause

- Observation of \alert{households} with multiple members,

\pause

- Communities composed of households, individuals. 


## Clustering may occur at multiple levels within a single dataset

```{r echo=FALSE, out.width='80%', fig.align="center", fig.cap="For repeated measures of individuals, nested within neighrborhoods, within a city. (From Merlo 2005 \\protect\\parencite{merlo2005})"}
include_local_graphics("images/merlo_variance_partition.png")
```


## Examples of Clustered Data/Mechanisms

- Repeated measures of individuals in a cohort study.

\pause

- Cross-sectional observation of flu transmission in households.

\pause

- Common social risks for non-communicable diseases in neighborhoods.

\pause

- Policies impacting cities, regions, etc (e.g. mask and vaccine mandates for COVID-19).


## Clustered data provide opportunities and challenges 

- \alert{Measure} variation in effects across units.
\pause
- \alert{Adjust} for unobserved or partially observed potential *confounders*.
\pause
- \alert{Predict} outcomes in new units where covariates are known but outcomes are unobseved.

## Taking multiple measures of the same individual allows us to control for unobserved confounding

```{r echo=FALSE, out.width='50%', fig.align="center", fig.cap="A within-subject case/control design."}
include_local_graphics("images/repeated_measure_eye.jpg")
```

## Making sense of \alert{spatial clustering} often requires us to think about the environment

```{r echo=FALSE, out.width='70%', fig.align="center", fig.cap="John Snow's classic map of Cholera Deaths in 1854"}
include_local_graphics("images/snow_cholera_map.jpg")
```

## Clustering of \alert{health behaviors} can impact disease risks

```{r echo=FALSE, out.width='70%', fig.align="center", fig.cap="Clustered non-vaccination dramatically increases measles outbreak risk (Figure from Masters et al. 2020 \\protect\\parencite{masters2020})"}
include_local_graphics("images/masters_nonvax_clustering.png")
```

# A re-introduction to Generalized Linear Models (GLMs) for clustered data

## Notation for a classic un-clustered GLM

Going to be seeing a lot of this:

- $y_{i} = \alpha + \beta x_i + \epsilon_i$

\pause

Where:

 - $y_i$ is \alert{continuous} outcome measure: height, BMI, etc.
 \pause
 - $\beta$ is risk associated with some kind of exposure
 \pause
 - $x_i \in [0,1]$ is an indicator of exposure.
 \pause
 - $\alpha$ is expected outcome when $x_i$ = 0
 \pause
 - $\epsilon_i$ are independently and identically distributed (i.i.d.) errors

## Independent errors

Classic assumption is that:

- $\epsilon_i \sim N(0, \sigma^2)$

\pause

In plain-ish English:

 - Observation $y_{ij}$ of individual $i$ is a function of $\alpha + \beta x_i$ and normally distributed \alert{errors} ($\epsilon_i$) with mean zero and variance $\sigma^2$.

\pause

Another way of writing it:

- $y_i \sim N(\alpha + \beta x_i, \sigma^2)$


## Three Approaches to Modeling Clustered Data

```{r echo=FALSE, out.width='70%', fig.align="center", fig.cap="Which door will you choose?"}
include_local_graphics("images/letsmakeadeal.jpg")
```


## Door #1: Ignore clustering and fit a normal GLM

- *Pool* data across all units, i.e. ignore clustering.

- i.e. fit model $y_{ij} = \alpha + \beta x_i + \epsilon_i$

\begin{center}
Is this a good idea? Why or why not?
\end{center}

## NO!

```{r echo=FALSE, out.width='80%', fig.align="center", fig.cap="Complete pooling ignores potential sources of observed and unobserved confounding."}
include_local_graphics("images/regression_dragon_1.jpg")
```

## Pooling clustered data violates assumption of independent errors

A \alert{pooled} model:

\begin{equation}
y_i = \alpha + \beta x + \epsilon_i
\end{equation}

\pause

- $y_i$ is a combination of systematic variation ($\alpha + \beta x$) and *uncorrelated* random noise ($\epsilon_i$) where:


\begin{equation}
i.i.d.\ \epsilon \sim Normal(0, \sigma^2)
\end{equation}

## Clustering may result in correlation between average differences from mean

```{r echo=FALSE, out.width='80%', fig.align="center", fig.cap="Complete pooling ignores potential sources of observed and unobserved confounding."}
include_local_graphics("images/merlo_variance_partition.png")
```


## Your \alert{residuals} should look like this

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width='70%', fig.cap="Example of residuals for model with clustered errors"}
require(ggplot2)
df <- data.frame(x = rnorm(1000))
g <- ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 0.1) +
  xlab("Distance from mean model prediction") +
  ylab("N")

plot(g)
```

## When you ignore clustering you may see something like:

```{r, echo=FALSE, out.width='70%', fig.cap="Clustered errors"}
require(ggplot2)
ind_cluster <- 100
ncluster <- 10
cluster_sigma <- 1
ind_sigma <- 0.2

cluster_ids <- sort(rep(1:ncluster, ind_cluster))

cluster_means <- rnorm(ncluster, sd = cluster_sigma)
ind_vals <- rnorm(n = length(cluster_ids), mean = cluster_means[cluster_ids], sd = ind_sigma)

df <- data.frame(x = ind_vals, cluster = cluster_ids)

g <- ggplot(df, aes(x = x, cluster = cluster_ids)) +
  geom_histogram(binwidth = 0.05) +
  xlab("Distance from mean") +
  ylab("N")

plot(g)
```

## Door #2: Fit a different model to each cluster

Fit *unpooled* model to each unit ($j$), assuming outcomes in each unit are independent:

- $y_{ij} = \alpha_j + \beta_j x_i + \epsilon_{ij}$

- $\epsilon_{ij} \sim N(0, \sigma_{j}^2)$

## More danger!

```{r echo=FALSE, out.width='80%', fig.align="center", fig.cap="Totally unpooled models run the risk of \\alert{overfitting} the data, particularly in small samples."}
include_local_graphics("images/regression_dino.jpg")
```

## Specific dangers of unpooled models

What else could go wrong here?

\pause

- Some units (e.g. counties) may have few observations, making *unpooled* models impractical

\pause

- We may want to allow some effect of exposure (e.g. having a basement) to be consistent across counties.

## Door #3: Partial Pooling!

- Allow effects to vary across clusters, but constrain them with a \alert{prior} distribution.

\pause

- This approach accommodates variation across units without assuming they have no similarity.

\pause

- More likely to make accurate \alert{out-of-sample} predictions than the fully-pooled or unpooled examples.

## Partial pooling = \alert{Smoothing}

```{r echo=FALSE, out.width='60%', fig.align="center", fig.cap="Both functions fit the data perfectly...which one should you prefer and why?"}
include_local_graphics("images/wiki_regularization.png")
```

## A \alert{hands-on} smoothing example

```{r echo=FALSE, out.width='55%', fig.align="center", fig.cap=""}
include_local_graphics("images/smoothing_app_front_page.png")
```

https://sph-umich.shinyapps.io/smoothing/


# Break!

## \alert{Hands-on} with some hierarchical models

```{r echo=FALSE, out.width='70%', fig.align="center", fig.cap=""}
include_local_graphics("images/hierarchical_app_front_page.png")
```

 https://sph-umich.shinyapps.io/hierarchical_models/

<!-- # Radon Example -->

<!-- ## Radon is a carcinogenic gas -->
<!-- \begin{figure} -->
<!--   \captionsetup{justification=justified} -->
<!--   \includegraphics[height=0.5\textwidth]{images/radon_entry.png} -->
<!--   \caption{Radon is a byproduct of decaying soil uranium.} -->
<!-- \end{figure} -->

<!-- ## Radon enters a house more easily when it is built into the ground -->
<!-- \begin{figure} -->
<!--   \captionsetup{justification=justified} -->
<!--   \includegraphics[height=0.6\textwidth]{images/basement_radon.png} -->
<!--   \caption{Ann Arbor is a radon hotspot!} -->
<!-- \end{figure} -->

<!-- ## Considerable geographic variation in radon potential -->
<!-- \begin{figure} -->
<!--   \captionsetup{justification=justified} -->
<!--   \includegraphics[height=0.6\textwidth]{images/radon_map.png} -->
<!--   \caption{Ann Arbor is a radon hotspot!} -->
<!-- \end{figure} -->

<!-- ## Trust me on this one... -->
<!-- \begin{figure} -->
<!--   \captionsetup{justification=justified} -->
<!--   \includegraphics[height=0.6\textwidth]{images/jz_radon.jpg} -->
<!--   \caption{My very own radon mitigation system.} -->
<!-- \end{figure} -->


<!-- ## What should a model that accounts for important sources of variation in household radon potential include? -->

<!-- \pause -->

<!-- - County-level variation in soil uranium. -->

<!-- - Whether or not the radon measurement was taken in a basement. -->


<!-- ## \alert{Random intercepts} account for county-level variation -->

<!-- Gelman [@Gelman2006] proposes a multi-level model to measure household radon in household $i$ in county $j$, $y_{ij}$: -->

<!-- - $y_{ij} \sim N(\alpha_j + \beta x_{ij}, \sigma^{2}_y)$, for $i = 1, \ldots, n_j$, $j=1,\ldots,J$ -->
<!-- \pause -->

<!-- Where: -->

<!-- - $\alpha_j$ is average, \alert{non-basement} radon measure at county level -->

<!-- - $\beta$ is \alert{fixed effect} measuring average change in radon level in houses with a basement. -->

<!-- - $\sigma^{2}_y$ represents \alert{within-county} variation in risk -->


<!-- ## Include predictors of county-level variation in second level -->

<!-- \alert{County-level random intercept} is a function of county soil uranium measure, $u_j$: -->

<!-- - $\alpha_j \sim N(\gamma_0 + \gamma_1 u_j, \sigma_{\alpha}^2)$, for $j = 1, \ldots, J$ -->

<!-- \pause -->

<!-- Where: -->

<!-- - $\gamma_0$ is expected \alert{household radon measure} when $u_j = 0$ -->
<!-- - $\gamma_1$ scales expected county-level uranium with $u_j$ -->
<!-- - $\sigma_{\alpha}^2$ is \alert{between-county} variation in radon risk not measured by $u_j$. -->

<!-- ## Putting it all together -->

<!-- \alert{County-level intercept} is a function of county soil uranium measure, $u_j$: -->

<!-- - $\alpha_j \sim N(\gamma_0 + \gamma_1 u_j, \sigma_{\alpha}^2)$ -->

<!-- \pause -->

<!-- \alert{Household-level} radon measure is a function of having a \alert{basement} and county-level intercept: -->

<!-- - $y_{ij} \sim N(\alpha_j + \beta x_{ij}, \sigma^{2}_y)$ -->

<!-- ## County-level radon levels vary with soil uranium measures -->

<!-- \begin{figure} -->
<!--   \captionsetup{justification=justified} -->
<!--   \includegraphics[height=0.5\textwidth]{images/gelman_county_uranium.png} -->
<!--   \caption{County-level intercept, $\alpha_j$, ($\pm 1$ standard error) as a function of county-level uranium.} -->
<!-- \end{figure} -->

<!-- ## Model predictions vs. radon measures by county -->

<!-- \begin{figure} -->
<!--   \captionsetup{justification=justified} -->
<!--   \includegraphics[height=0.4\textwidth]{images/gelman_basement_effect.png} -->
<!--   \caption{Multi-level regression line, $y = \alpha_j + \beta x$, from 8 Minnesota counties. Unpooled estimates = light grey line; Totally pooled estimates = dashed grey line.} -->
<!-- \end{figure} -->


<!-- # An Exercise -->

<!-- ## How good is your memory? -->

<!-- - I'll read a sequence of 9 numbers. -->

<!-- \pause -->

<!-- - We'll wait 15 seconds after I stop reading the numbers, and then write down as many as you can remember. -->

<!-- ## Count how many you got correct. -->

<!-- The numbers: -->

<!-- - `r sample(1:30, 9, replace = FALSE)` -->


<!-- On your paper, write down how many you got correct next to "t=1". -->

<!-- ## Let's do it again! -->

<!-- \pause -->

<!-- The numbers: -->

<!-- - `r sample(1:30, 9, replace = FALSE)` -->

<!-- Count up how many you got correct this time around, and write it down next to "t=2". -->

<!-- ## Submit your responses! -->

<!-- https://bit.ly/2r82C6e -->


<!-- ## Break! -->

<!-- After the break, we'll talk about what we can learn from these data and how to use a hierarchical model to extract the relevant information from the data. -->

<!-- ## So why did we do that? -->

<!-- We'll devise and test a model to answer two questions: -->

<!-- 1. Does the \alert{average} individual remember more numbers the second time? -->

<!-- \pause -->

<!-- 2. How does \alert{your} memory compare everyone else's? -->



<!-- ## Learning from repeated measures -->

<!-- - Need to adjust for possible confounding variables at the individual level so that we can assume outcomes are \alert{conditionally independent}. -->

<!-- \pause -->

<!-- - Have to use an \alert{partial pooling} approach to learn from individual-level data without overfitting to small numbers of observations. -->

<!-- ## Choosing the outcome model -->

<!-- - Outcome is the number of digits, $y_i$, out of $k$ total each person remembered. -->

<!-- \pause -->

<!-- - We want to estimate the \alert{probability}, $p_i$ each individual remembered $y_i$ out of $k$ digits. -->

<!-- \pause -->

<!-- - What distribution does this imply? -->

<!-- ## The number of items remembered has a \alert{binomial} distribution -->

<!-- \begin{equation*} -->
<!-- y_{it} \sim Binomial(k_t, p_{it}) -->
<!-- \end{equation*} -->

<!-- A binomial distribution describes the probability of observing $y_{it}$ successes out of $k_t$ choices, with individual probability of success $p_{it}$. -->

<!-- ## We can use logistic regression to estimate the probability of remembering each digit -->

<!-- \begin{equation*} -->
<!-- logit(\hat{p}_{it}) = \alpha + \beta x_{it} -->
<!-- \end{equation*} -->

<!-- Where: -->

<!-- - $\hat{p}_{it}$ is the estimated probability of remembering each digit. -->
<!--  - $x_{it} = 0$ when $t = 1$; $x_{it} = 1$ when $t = 2$. -->
<!--  - $\alpha$ is the \alert{logit} of success when $k=7$. -->
<!--  - $\alpha + \beta$ is the logit of success when $k=9$. -->

<!-- ## Inverse Logit Function, $logit^-1(x)$ -->

<!-- ```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Mapping logit to probability"} -->
<!-- require(ggplot2) -->
<!-- x <- seq(from = -5, to = 5, by = 0.1) -->
<!-- y <- plogis(x) -->
<!-- df <- data.frame(x = x, y = y) -->
<!-- g <- ggplot(df, aes(x = x, y = y)) + -->
<!--   geom_point() + -->
<!--   xlab("Logit of success probability") + -->
<!--   ylab("Success Probability") -->
<!-- plot(g) -->
<!-- ``` -->

<!-- ## Use \alert{hierarchical} model to capture individual-level variation in skill -->

<!-- \begin{equation*} -->
<!-- logit(\hat{p}_{it}) = \alpha_i + \beta x_{it} -->
<!-- \end{equation*} -->

<!-- Where: -->

<!-- - $\alpha_i \sim N(\alpha_mu, sigma_{\alpha})$ -->

<!-- So that: -->

<!-- - $(p_i | k = 7) = logit^{-1}(\alpha_i)$ -->

<!-- And: -->

<!--  - $(p_i | k = 9) = logit^{-1}(\alpha_i + \beta)$ -->


<!-- ## How good of a job does it actually do? -->

<!-- \begin{figure} -->
<!--   \captionsetup{justification=justified} -->
<!--   \includegraphics[height=0.6\textwidth]{images/will_it_blend.jpg} -->
<!--   \caption{Time to find out...} -->
<!-- \end{figure} -->


<!-- ## Discussion Questions -->

<!-- - Did the model do a reasonable job of describing the data? -->

<!-- \pause -->

<!-- - Can we interpret the effect of $\beta$ as being a measurement of the causal effect of increasing the number of digits? -->

<!-- \pause -->

<!-- - What could we do to improve the model? -->

## References {.allowframebreaks}

