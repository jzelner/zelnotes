{"title":"Models for clustered data","markdown":{"yaml":{"author":["Jon Zelner"],"header-includes":["\\usepackage{float}","\\usepackage{wrapfig}","\\usepackage{graphicx}","\\usepackage{appendixnumberbeamer}","\\usepackage{caption}","\\captionsetup[figure]{labelformat=empty}"],"title":"Models for clustered data","institute":"EPID 601 \\newline University of Michigan School of Public Health  \\newline \\newline `jzelner@umich.edu` \\newline `epibayes.io`","fontsize":"12pt","output":{"beamer_presentation":{"theme":"metropolis","latex_engine":"xelatex","citation_package":"biblatex"}},"date":"\\today","bibliography":"/Users/jzelner/repos/bibtex-library/jz_library.bib"},"headingText":"This function uses the `here` package to make it easy to include graphics in a directory","containsRefs":false,"markdown":"\n\n```{r setup, echo=FALSE,warning=FALSE,message=FALSE}\nknitr::opts_knit$set(root.dir = rprojroot::find_root(\".git/index\"))\n## relative to the project root\ninclude_local_graphics <- function(x) {\n  knitr::include_graphics(here::here(x))\n}\n```\n\n## Roadmap\n\n- How can we model clustered data? What are the pitfalls of different approaches?\n\n\\pause\n\n- Smoothing is at the heart of modeling clustered data: \\alert{Hands-on}\n\n\\pause\n\n- Break!\n\n\\pause\n\n- How quickly can you break a regression model? A hierarchical modeling \\alert{hands-on}.\n\n## It's that time of year.\n\n```{r echo=FALSE, out.width='50%', fig.align=\"center\", fig.cap=\"That November feeling...\"}\ninclude_local_graphics(\"images/sleepy_bee.jpg\")\n```\n\n## Exposure clustering is a fundamental epidemiological problem\n\nOn your own:\n\n - Where do we find clustering in real-world data?\n\n\\pause\n\nIn small groups:\n\n- What \\alert{mechanisms} induce clustering?\n\n\\pause\n\n- How do you think \\alert{ignoring} clustering impact the conclusions we draw from data?\n\n\\pause\n\n- What are some implications of clustering in the context of \\alert{applied epidemiology}?\n\n\n## Types of clustered data\n\n- Repeated measures of \\alert{individuals}, \n\n\\pause\n\n- Observation of \\alert{households} with multiple members,\n\n\\pause\n\n- Communities composed of households, individuals. \n\n\n## Clustering may occur at multiple levels within a single dataset\n\n```{r echo=FALSE, out.width='80%', fig.align=\"center\", fig.cap=\"For repeated measures of individuals, nested within neighrborhoods, within a city. (From Merlo 2005 \\\\protect\\\\parencite{merlo2005})\"}\ninclude_local_graphics(\"images/merlo_variance_partition.png\")\n```\n\n\n## Examples of Clustered Data/Mechanisms\n\n- Repeated measures of individuals in a cohort study.\n\n\\pause\n\n- Cross-sectional observation of flu transmission in households.\n\n\\pause\n\n- Common social risks for non-communicable diseases in neighborhoods.\n\n\\pause\n\n- Policies impacting cities, regions, etc (e.g. mask and vaccine mandates for COVID-19).\n\n\n## Clustered data provide opportunities and challenges \n\n- \\alert{Measure} variation in effects across units.\n\\pause\n- \\alert{Adjust} for unobserved or partially observed potential *confounders*.\n\\pause\n- \\alert{Predict} outcomes in new units where covariates are known but outcomes are unobseved.\n\n## Taking multiple measures of the same individual allows us to control for unobserved confounding\n\n```{r echo=FALSE, out.width='50%', fig.align=\"center\", fig.cap=\"A within-subject case/control design.\"}\ninclude_local_graphics(\"images/repeated_measure_eye.jpg\")\n```\n\n## Making sense of \\alert{spatial clustering} often requires us to think about the environment\n\n```{r echo=FALSE, out.width='70%', fig.align=\"center\", fig.cap=\"John Snow's classic map of Cholera Deaths in 1854\"}\ninclude_local_graphics(\"images/snow_cholera_map.jpg\")\n```\n\n## Clustering of \\alert{health behaviors} can impact disease risks\n\n```{r echo=FALSE, out.width='70%', fig.align=\"center\", fig.cap=\"Clustered non-vaccination dramatically increases measles outbreak risk (Figure from Masters et al. 2020 \\\\protect\\\\parencite{masters2020})\"}\ninclude_local_graphics(\"images/masters_nonvax_clustering.png\")\n```\n\n# A re-introduction to Generalized Linear Models (GLMs) for clustered data\n\n## Notation for a classic un-clustered GLM\n\nGoing to be seeing a lot of this:\n\n- $y_{i} = \\alpha + \\beta x_i + \\epsilon_i$\n\n\\pause\n\nWhere:\n\n - $y_i$ is \\alert{continuous} outcome measure: height, BMI, etc.\n \\pause\n - $\\beta$ is risk associated with some kind of exposure\n \\pause\n - $x_i \\in [0,1]$ is an indicator of exposure.\n \\pause\n - $\\alpha$ is expected outcome when $x_i$ = 0\n \\pause\n - $\\epsilon_i$ are independently and identically distributed (i.i.d.) errors\n\n## Independent errors\n\nClassic assumption is that:\n\n- $\\epsilon_i \\sim N(0, \\sigma^2)$\n\n\\pause\n\nIn plain-ish English:\n\n - Observation $y_{ij}$ of individual $i$ is a function of $\\alpha + \\beta x_i$ and normally distributed \\alert{errors} ($\\epsilon_i$) with mean zero and variance $\\sigma^2$.\n\n\\pause\n\nAnother way of writing it:\n\n- $y_i \\sim N(\\alpha + \\beta x_i, \\sigma^2)$\n\n\n## Three Approaches to Modeling Clustered Data\n\n```{r echo=FALSE, out.width='70%', fig.align=\"center\", fig.cap=\"Which door will you choose?\"}\ninclude_local_graphics(\"images/letsmakeadeal.jpg\")\n```\n\n\n## Door #1: Ignore clustering and fit a normal GLM\n\n- *Pool* data across all units, i.e. ignore clustering.\n\n- i.e. fit model $y_{ij} = \\alpha + \\beta x_i + \\epsilon_i$\n\n\\begin{center}\nIs this a good idea? Why or why not?\n\\end{center}\n\n## NO!\n\n```{r echo=FALSE, out.width='80%', fig.align=\"center\", fig.cap=\"Complete pooling ignores potential sources of observed and unobserved confounding.\"}\ninclude_local_graphics(\"images/regression_dragon_1.jpg\")\n```\n\n## Pooling clustered data violates assumption of independent errors\n\nA \\alert{pooled} model:\n\n\\begin{equation}\ny_i = \\alpha + \\beta x + \\epsilon_i\n\\end{equation}\n\n\\pause\n\n- $y_i$ is a combination of systematic variation ($\\alpha + \\beta x$) and *uncorrelated* random noise ($\\epsilon_i$) where:\n\n\n\\begin{equation}\ni.i.d.\\ \\epsilon \\sim Normal(0, \\sigma^2)\n\\end{equation}\n\n## Clustering may result in correlation between average differences from mean\n\n```{r echo=FALSE, out.width='80%', fig.align=\"center\", fig.cap=\"Complete pooling ignores potential sources of observed and unobserved confounding.\"}\ninclude_local_graphics(\"images/merlo_variance_partition.png\")\n```\n\n\n## Your \\alert{residuals} should look like this\n\n```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width='70%', fig.cap=\"Example of residuals for model with clustered errors\"}\nrequire(ggplot2)\ndf <- data.frame(x = rnorm(1000))\ng <- ggplot(df, aes(x = x)) +\n  geom_histogram(binwidth = 0.1) +\n  xlab(\"Distance from mean model prediction\") +\n  ylab(\"N\")\n\nplot(g)\n```\n\n## When you ignore clustering you may see something like:\n\n```{r, echo=FALSE, out.width='70%', fig.cap=\"Clustered errors\"}\nrequire(ggplot2)\nind_cluster <- 100\nncluster <- 10\ncluster_sigma <- 1\nind_sigma <- 0.2\n\ncluster_ids <- sort(rep(1:ncluster, ind_cluster))\n\ncluster_means <- rnorm(ncluster, sd = cluster_sigma)\nind_vals <- rnorm(n = length(cluster_ids), mean = cluster_means[cluster_ids], sd = ind_sigma)\n\ndf <- data.frame(x = ind_vals, cluster = cluster_ids)\n\ng <- ggplot(df, aes(x = x, cluster = cluster_ids)) +\n  geom_histogram(binwidth = 0.05) +\n  xlab(\"Distance from mean\") +\n  ylab(\"N\")\n\nplot(g)\n```\n\n## Door #2: Fit a different model to each cluster\n\nFit *unpooled* model to each unit ($j$), assuming outcomes in each unit are independent:\n\n- $y_{ij} = \\alpha_j + \\beta_j x_i + \\epsilon_{ij}$\n\n- $\\epsilon_{ij} \\sim N(0, \\sigma_{j}^2)$\n\n## More danger!\n\n```{r echo=FALSE, out.width='80%', fig.align=\"center\", fig.cap=\"Totally unpooled models run the risk of \\\\alert{overfitting} the data, particularly in small samples.\"}\ninclude_local_graphics(\"images/regression_dino.jpg\")\n```\n\n## Specific dangers of unpooled models\n\nWhat else could go wrong here?\n\n\\pause\n\n- Some units (e.g. counties) may have few observations, making *unpooled* models impractical\n\n\\pause\n\n- We may want to allow some effect of exposure (e.g. having a basement) to be consistent across counties.\n\n## Door #3: Partial Pooling!\n\n- Allow effects to vary across clusters, but constrain them with a \\alert{prior} distribution.\n\n\\pause\n\n- This approach accommodates variation across units without assuming they have no similarity.\n\n\\pause\n\n- More likely to make accurate \\alert{out-of-sample} predictions than the fully-pooled or unpooled examples.\n\n## Partial pooling = \\alert{Smoothing}\n\n```{r echo=FALSE, out.width='60%', fig.align=\"center\", fig.cap=\"Both functions fit the data perfectly...which one should you prefer and why?\"}\ninclude_local_graphics(\"images/wiki_regularization.png\")\n```\n\n## A \\alert{hands-on} smoothing example\n\n```{r echo=FALSE, out.width='55%', fig.align=\"center\", fig.cap=\"\"}\ninclude_local_graphics(\"images/smoothing_app_front_page.png\")\n```\n\nhttps://sph-umich.shinyapps.io/smoothing/\n\n\n# Break!\n\n## \\alert{Hands-on} with some hierarchical models\n\n```{r echo=FALSE, out.width='70%', fig.align=\"center\", fig.cap=\"\"}\ninclude_local_graphics(\"images/hierarchical_app_front_page.png\")\n```\n\n https://sph-umich.shinyapps.io/hierarchical_models/\n\n<!-- # Radon Example -->\n\n<!-- ## Radon is a carcinogenic gas -->\n<!-- \\begin{figure} -->\n<!--   \\captionsetup{justification=justified} -->\n<!--   \\includegraphics[height=0.5\\textwidth]{images/radon_entry.png} -->\n<!--   \\caption{Radon is a byproduct of decaying soil uranium.} -->\n<!-- \\end{figure} -->\n\n<!-- ## Radon enters a house more easily when it is built into the ground -->\n<!-- \\begin{figure} -->\n<!--   \\captionsetup{justification=justified} -->\n<!--   \\includegraphics[height=0.6\\textwidth]{images/basement_radon.png} -->\n<!--   \\caption{Ann Arbor is a radon hotspot!} -->\n<!-- \\end{figure} -->\n\n<!-- ## Considerable geographic variation in radon potential -->\n<!-- \\begin{figure} -->\n<!--   \\captionsetup{justification=justified} -->\n<!--   \\includegraphics[height=0.6\\textwidth]{images/radon_map.png} -->\n<!--   \\caption{Ann Arbor is a radon hotspot!} -->\n<!-- \\end{figure} -->\n\n<!-- ## Trust me on this one... -->\n<!-- \\begin{figure} -->\n<!--   \\captionsetup{justification=justified} -->\n<!--   \\includegraphics[height=0.6\\textwidth]{images/jz_radon.jpg} -->\n<!--   \\caption{My very own radon mitigation system.} -->\n<!-- \\end{figure} -->\n\n\n<!-- ## What should a model that accounts for important sources of variation in household radon potential include? -->\n\n<!-- \\pause -->\n\n<!-- - County-level variation in soil uranium. -->\n\n<!-- - Whether or not the radon measurement was taken in a basement. -->\n\n\n<!-- ## \\alert{Random intercepts} account for county-level variation -->\n\n<!-- Gelman [@Gelman2006] proposes a multi-level model to measure household radon in household $i$ in county $j$, $y_{ij}$: -->\n\n<!-- - $y_{ij} \\sim N(\\alpha_j + \\beta x_{ij}, \\sigma^{2}_y)$, for $i = 1, \\ldots, n_j$, $j=1,\\ldots,J$ -->\n<!-- \\pause -->\n\n<!-- Where: -->\n\n<!-- - $\\alpha_j$ is average, \\alert{non-basement} radon measure at county level -->\n\n<!-- - $\\beta$ is \\alert{fixed effect} measuring average change in radon level in houses with a basement. -->\n\n<!-- - $\\sigma^{2}_y$ represents \\alert{within-county} variation in risk -->\n\n\n<!-- ## Include predictors of county-level variation in second level -->\n\n<!-- \\alert{County-level random intercept} is a function of county soil uranium measure, $u_j$: -->\n\n<!-- - $\\alpha_j \\sim N(\\gamma_0 + \\gamma_1 u_j, \\sigma_{\\alpha}^2)$, for $j = 1, \\ldots, J$ -->\n\n<!-- \\pause -->\n\n<!-- Where: -->\n\n<!-- - $\\gamma_0$ is expected \\alert{household radon measure} when $u_j = 0$ -->\n<!-- - $\\gamma_1$ scales expected county-level uranium with $u_j$ -->\n<!-- - $\\sigma_{\\alpha}^2$ is \\alert{between-county} variation in radon risk not measured by $u_j$. -->\n\n<!-- ## Putting it all together -->\n\n<!-- \\alert{County-level intercept} is a function of county soil uranium measure, $u_j$: -->\n\n<!-- - $\\alpha_j \\sim N(\\gamma_0 + \\gamma_1 u_j, \\sigma_{\\alpha}^2)$ -->\n\n<!-- \\pause -->\n\n<!-- \\alert{Household-level} radon measure is a function of having a \\alert{basement} and county-level intercept: -->\n\n<!-- - $y_{ij} \\sim N(\\alpha_j + \\beta x_{ij}, \\sigma^{2}_y)$ -->\n\n<!-- ## County-level radon levels vary with soil uranium measures -->\n\n<!-- \\begin{figure} -->\n<!--   \\captionsetup{justification=justified} -->\n<!--   \\includegraphics[height=0.5\\textwidth]{images/gelman_county_uranium.png} -->\n<!--   \\caption{County-level intercept, $\\alpha_j$, ($\\pm 1$ standard error) as a function of county-level uranium.} -->\n<!-- \\end{figure} -->\n\n<!-- ## Model predictions vs. radon measures by county -->\n\n<!-- \\begin{figure} -->\n<!--   \\captionsetup{justification=justified} -->\n<!--   \\includegraphics[height=0.4\\textwidth]{images/gelman_basement_effect.png} -->\n<!--   \\caption{Multi-level regression line, $y = \\alpha_j + \\beta x$, from 8 Minnesota counties. Unpooled estimates = light grey line; Totally pooled estimates = dashed grey line.} -->\n<!-- \\end{figure} -->\n\n\n<!-- # An Exercise -->\n\n<!-- ## How good is your memory? -->\n\n<!-- - I'll read a sequence of 9 numbers. -->\n\n<!-- \\pause -->\n\n<!-- - We'll wait 15 seconds after I stop reading the numbers, and then write down as many as you can remember. -->\n\n<!-- ## Count how many you got correct. -->\n\n<!-- The numbers: -->\n\n<!-- - `r sample(1:30, 9, replace = FALSE)` -->\n\n\n<!-- On your paper, write down how many you got correct next to \"t=1\". -->\n\n<!-- ## Let's do it again! -->\n\n<!-- \\pause -->\n\n<!-- The numbers: -->\n\n<!-- - `r sample(1:30, 9, replace = FALSE)` -->\n\n<!-- Count up how many you got correct this time around, and write it down next to \"t=2\". -->\n\n<!-- ## Submit your responses! -->\n\n<!-- https://bit.ly/2r82C6e -->\n\n\n<!-- ## Break! -->\n\n<!-- After the break, we'll talk about what we can learn from these data and how to use a hierarchical model to extract the relevant information from the data. -->\n\n<!-- ## So why did we do that? -->\n\n<!-- We'll devise and test a model to answer two questions: -->\n\n<!-- 1. Does the \\alert{average} individual remember more numbers the second time? -->\n\n<!-- \\pause -->\n\n<!-- 2. How does \\alert{your} memory compare everyone else's? -->\n\n\n\n<!-- ## Learning from repeated measures -->\n\n<!-- - Need to adjust for possible confounding variables at the individual level so that we can assume outcomes are \\alert{conditionally independent}. -->\n\n<!-- \\pause -->\n\n<!-- - Have to use an \\alert{partial pooling} approach to learn from individual-level data without overfitting to small numbers of observations. -->\n\n<!-- ## Choosing the outcome model -->\n\n<!-- - Outcome is the number of digits, $y_i$, out of $k$ total each person remembered. -->\n\n<!-- \\pause -->\n\n<!-- - We want to estimate the \\alert{probability}, $p_i$ each individual remembered $y_i$ out of $k$ digits. -->\n\n<!-- \\pause -->\n\n<!-- - What distribution does this imply? -->\n\n<!-- ## The number of items remembered has a \\alert{binomial} distribution -->\n\n<!-- \\begin{equation*} -->\n<!-- y_{it} \\sim Binomial(k_t, p_{it}) -->\n<!-- \\end{equation*} -->\n\n<!-- A binomial distribution describes the probability of observing $y_{it}$ successes out of $k_t$ choices, with individual probability of success $p_{it}$. -->\n\n<!-- ## We can use logistic regression to estimate the probability of remembering each digit -->\n\n<!-- \\begin{equation*} -->\n<!-- logit(\\hat{p}_{it}) = \\alpha + \\beta x_{it} -->\n<!-- \\end{equation*} -->\n\n<!-- Where: -->\n\n<!-- - $\\hat{p}_{it}$ is the estimated probability of remembering each digit. -->\n<!--  - $x_{it} = 0$ when $t = 1$; $x_{it} = 1$ when $t = 2$. -->\n<!--  - $\\alpha$ is the \\alert{logit} of success when $k=7$. -->\n<!--  - $\\alpha + \\beta$ is the logit of success when $k=9$. -->\n\n<!-- ## Inverse Logit Function, $logit^-1(x)$ -->\n\n<!-- ```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = \"Mapping logit to probability\"} -->\n<!-- require(ggplot2) -->\n<!-- x <- seq(from = -5, to = 5, by = 0.1) -->\n<!-- y <- plogis(x) -->\n<!-- df <- data.frame(x = x, y = y) -->\n<!-- g <- ggplot(df, aes(x = x, y = y)) + -->\n<!--   geom_point() + -->\n<!--   xlab(\"Logit of success probability\") + -->\n<!--   ylab(\"Success Probability\") -->\n<!-- plot(g) -->\n<!-- ``` -->\n\n<!-- ## Use \\alert{hierarchical} model to capture individual-level variation in skill -->\n\n<!-- \\begin{equation*} -->\n<!-- logit(\\hat{p}_{it}) = \\alpha_i + \\beta x_{it} -->\n<!-- \\end{equation*} -->\n\n<!-- Where: -->\n\n<!-- - $\\alpha_i \\sim N(\\alpha_mu, sigma_{\\alpha})$ -->\n\n<!-- So that: -->\n\n<!-- - $(p_i | k = 7) = logit^{-1}(\\alpha_i)$ -->\n\n<!-- And: -->\n\n<!--  - $(p_i | k = 9) = logit^{-1}(\\alpha_i + \\beta)$ -->\n\n\n<!-- ## How good of a job does it actually do? -->\n\n<!-- \\begin{figure} -->\n<!--   \\captionsetup{justification=justified} -->\n<!--   \\includegraphics[height=0.6\\textwidth]{images/will_it_blend.jpg} -->\n<!--   \\caption{Time to find out...} -->\n<!-- \\end{figure} -->\n\n\n<!-- ## Discussion Questions -->\n\n<!-- - Did the model do a reasonable job of describing the data? -->\n\n<!-- \\pause -->\n\n<!-- - Can we interpret the effect of $\\beta$ as being a measurement of the causal effect of increasing the number of digits? -->\n\n<!-- \\pause -->\n\n<!-- - What could we do to improve the model? -->\n\n## References {.allowframebreaks}\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":{"beamer_presentation":{"theme":"metropolis","latex_engine":"xelatex","citation_package":"biblatex"}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"guest_lecture_glmm.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","author":["Jon Zelner"],"header-includes":["\\usepackage{float}","\\usepackage{wrapfig}","\\usepackage{graphicx}","\\usepackage{appendixnumberbeamer}","\\usepackage{caption}","\\captionsetup[figure]{labelformat=empty}"],"title":"Models for clustered data","institute":"EPID 601 \\newline University of Michigan School of Public Health  \\newline \\newline `jzelner@umich.edu` \\newline `epibayes.io`","fontsize":"12pt","date":"\\today","bibliography":["/Users/jzelner/repos/bibtex-library/jz_library.bib"]},"extensions":{"book":{"multiFile":true}}}}}