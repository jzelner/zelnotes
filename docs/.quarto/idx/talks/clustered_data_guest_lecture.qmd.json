{"title":"Clustering!","markdown":{"yaml":{"pagetitle":"Clustering!","format":{"revealjs":{"show-notes":false,"theme":["solarized","styles.scss"],"incremental":true,"bibliography":"/Users/jzelner/repos/bibtex-library/jz_library.bib"}}},"headingText":"Clustering!","containsRefs":false,"markdown":"\n\n\n<div class=\"paddeddiv\">\n  <p style=\"font-size:0.5em; text-align: left;\">\n      University of Michigan School of Public Health\n      <br><br>\n      Jon Zelner  \n      `jzelner@umich.edu`  \n      [`epibayes.io`](https://epibayes.io) \n    </p>\n</div>\n\n<div class=\"itemr\" id=\"footerDiv\"><img id=\"footerImg\" src=\"../images/epid_logo.png\" width=\"400\"></div>\n\n## Agenda\n\n- What is clustering and why does it matter?\n\n- How can we <span class=\"alert\">model </span> clustered data? \n\n- What are the pitfalls of different approaches?\n\n- <span class=\"alert\">Break!</span>\n\n- How quickly can you break a regression model? A  <span class=\"alert\">hands-on</span> hierarchical modeling activity.\n\n## It's that time of year.\n\n![That November feeling...](../images/sleepy_bee.jpg)\n\n## What are we talking about when we talk about <span class=\"alert\">clustered data</span>?\n\n- Repeated measures of individuals in a cohort study.\n\n- Cross-sectional observation of flu transmission in households.\n\n- Common social risks for non-communicable diseases in neighborhoods.\n\n- Policies impacting cities, regions, etc (e.g. mask and vaccine mandates for COVID-19).\n\n## Clustering may occur at multiple levels within a single dataset\n\n![Variation in repeated measures of individuals, nested within neighrborhoods, within a city.](../images/merlo_variance_partition.png)\n\n## Clustered data provide opportunities and challenges \n\n- Measure variation in effects across units.\n\n- Adjust for unobserved or partially observed potential *confounders*.\n\n- Predict outcomes in new units where covariates are known but outcomes are unobseved.\n\n## Taking multiple measures of the same individual allows us to control for unobserved confounding\n\n![Perfect setting for a within-subject case-control ðŸ‘€](../images/repeated_measure_eye.jpg)\n\n## Making sense of <span class=\"alert\">spatial clustering</span> often requires that we think about the environment\n\n![John Snow's classic map of Cholera deaths in London's Golden Square in 1854](../images/snow_cholera_map.jpg)\n\n## Clustering of <span class=\"alert\">health behaviors</span> can impact disease risks\n\n![Spatially clustered non-vaccination dramatically increases measles outbreak risk (Figure from Masters et al. 2020)](../images/masters_nonvax_clustering.png)\n\n## What makes clustering such a <span class=\"alert\">fundamental</span> epidemiological problem?\n\nOn your own, in this [google doc](https://docs.google.com/document/d/1XP-FOv3gEV2sOZVtFU970GUuIoKLVrsSyCnqDcTAc4A/edit?usp=sharing):\n\n- What <span class=\"alert\">mechanisms</span> make clustering happen for an applied problem you care about? \n\n- Identify the outcome of interest and the levels at which clustering of risk are likely to occur.\n\n```{r}\nlibrary(countdown)\ncountdown(minutes = 6, warn_when = 120, seconds = 0, play_sound=TRUE)\n```\n\n## In small groups:\n\n- Share your outcome of interest and likely mechanisms with your partners.\n\n- How do you think <span class=\"alert\">ignoring</span> clustering can impact conclusions we draw from data? \n\n- How might policies and interventions based on models that ignore clustering go wrong? Can you come up with specific examples?\n\n```{r}\nlibrary(countdown)\ncountdown(minutes = 10, warn_when = 120, seconds = 0, play_sound=TRUE)\n```\n\n\n# A re-introduction to Generalized Linear Models (GLMs) for clustered data\n\n## Notation for a classic un-clustered GLM\n\nGoing to be seeing a lot of this:\n\n- $y_{i} = \\alpha + \\beta x_i + \\epsilon_i$\n\n\\pause\n\nWhere:\n\n - $y_i$ is \\alert{continuous} outcome measure: height, BMI, etc.\n - $\\beta$ is risk associated with some kind of exposure\n - $x_i \\in [0,1]$ is an indicator of exposure.\n - $\\alpha$ is expected outcome when $x_i$ = 0\n - $\\epsilon_i$ are independently and identically distributed (i.i.d.) errors\n\n## Independent errors\n\nClassic assumption is that:\n\n- $\\epsilon_i \\sim N(0, \\sigma^2)$\n\nIn plain-ish English:\n\n - Observation $y_{ij}$ of individual $i$ is a function of $\\alpha + \\beta x_i$ and normally distributed \\alert{errors} ($\\epsilon_i$) with mean zero and variance $\\sigma^2$.\n\n\nAnother way of writing it:\n\n- $y_i \\sim N(\\alpha + \\beta x_i, \\sigma^2)$\n\n\n## Three Approaches to Modeling Clustered Data\n\n![Which door will you choose?](../images/goat_switch.gif)\n\n\n## Door #1: Ignore clustering and fit a normal GLM\n\n- *Pool* data across all units, i.e. ignore clustering.\n\n- i.e. fit model $y_{ij} = \\alpha + \\beta x_i + \\epsilon_i$\n\n- Is this a good idea? Why or why not?\n\n## <span class=\"alert\">NO!</span>\n\n![Complete pooling ignores potential sources of observed and unobserved confounding.](../images/regression_dragon_1.jpg)\n\n\n## Pooling clustered data violates assumption of independent errors\n\nA <span class=\"alert\">fully pooled</span> model:\n\n- $y_i = \\alpha + \\beta x + \\epsilon_i$\n\n\n- $y_i$ is a combination of systematic variation ($\\alpha + \\beta x$) and *uncorrelated* random noise ($\\epsilon_i$) where:\n\n\n- $i.i.d.\\ \\epsilon \\sim Normal(0, \\sigma^2)$\n\n## Clustering may result in correlation between average differences from mean\n\n![Complete pooling ignores potential sources of observed and unobserved confounding.](../images/merlo_variance_partition.png)\n\n\n## Your <span class=\"alert\">residuals</span> should look like this\n\n```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width='70%', fig.cap=\"Example of residuals for model with clustered errors\"}\nrequire(ggplot2)\ndf <- data.frame(x = rnorm(1000))\ng <- ggplot(df, aes(x = x)) +\n  geom_histogram(binwidth = 0.1) +\n  xlab(\"Distance from mean model prediction\") +\n  ylab(\"N\")\n\nplot(g)\n```\n\n## When you ignore clustering you may see something like:\n\n```{r, echo=FALSE, out.width='70%', fig.cap=\"Clustered errors\"}\nrequire(ggplot2)\nind_cluster <- 100\nncluster <- 10\ncluster_sigma <- 1\nind_sigma <- 0.2\n\ncluster_ids <- sort(rep(1:ncluster, ind_cluster))\n\ncluster_means <- rnorm(ncluster, sd = cluster_sigma)\nind_vals <- rnorm(n = length(cluster_ids), mean = cluster_means[cluster_ids], sd = ind_sigma)\n\ndf <- data.frame(x = ind_vals, cluster = cluster_ids)\n\ng <- ggplot(df, aes(x = x, cluster = cluster_ids)) +\n  geom_histogram(binwidth = 0.05) +\n  xlab(\"Distance from mean\") +\n  ylab(\"N\")\n\nplot(g)\n```\n\n## Door #2: Fit a different model to each cluster\n\nFit *unpooled* model to each unit ($j$), assuming outcomes in each unit are independent:\n\n- $y_{ij} = \\alpha_j + \\beta_j x_i + \\epsilon_{ij}$\n\n- $\\epsilon_{ij} \\sim N(0, \\sigma_{j}^2)$\n\n## More danger!\n\n![Totally unpooled models run the risk of overfitting the data, particularly for small samples.](../images/regression_dino.jpg)\n\n## Specific dangers of unpooled models\n\nWhat else could go wrong here?\n\n- Some units (e.g. counties) may have few observations, making *unpooled* models impractical\n\n- We may want to allow some effect of exposure (e.g. having a basement) to be consistent across counties.\n\n## Door #3: Partial Pooling!\n\n- Allow effects to vary across clusters, but constrain them with a \\alert{prior} distribution.\n\n- This approach accommodates variation across units without assuming they have no similarity.\n\n- More likely to make accurate \\alert{out-of-sample} predictions than the fully-pooled or unpooled examples.\n\n## Partial pooling = <span class=\"alert\">Smoothing</span>\n\n![Both functions fit the data perfectly...which one should you prefer and why?](../images/wiki_regularization.png)\n\n# Break!\n\n```{r}\nlibrary(countdown)\ncountdown(minutes = 5, warn_when = 60, seconds = 0, play_sound=TRUE)\n```\n\n## <span class=\"alert\">Hands-on</span> ðŸ‘‹ with some hierarchical models\n\n![](../images/hierarchical_app_front_page.png)\n\n[https://sph-umich.shinyapps.io/hierarchical_models/](https://sph-umich.shinyapps.io/hierarchical_models/)\n\n# Thanks!"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":true,"output-file":"clustered_data_guest_lecture.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.313","auto-stretch":true,"pagetitle":"Clustering!","showNotes":false,"theme":["solarized","styles.scss"],"bibliography":["/Users/jzelner/repos/bibtex-library/jz_library.bib"]}}}}