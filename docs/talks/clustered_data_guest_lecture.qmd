---
pagetitle: "Clustering!"
format: 
    revealjs:
        show-notes: false
        theme: [solarized, styles.scss]
        incremental: true
        bibliography: /Users/jzelner/repos/bibtex-library/jz_library.bib
        #csl: "../../assets/american-journal-of-epidemiology.csl"
---

## Clustering!

<div class="paddeddiv">
  <p style="font-size:0.5em; text-align: left;">
      University of Michigan School of Public Health
      <br><br>
      Jon Zelner  
      `jzelner@umich.edu`  
      [`epibayes.io`](https://epibayes.io) 
    </p>
</div>

<div class="itemr" id="footerDiv"><img id="footerImg" src="../images/epid_logo.png" width="400"></div>

## Agenda

- What is clustering and why does it matter?

- How can we <span class="alert">model </span> clustered data? 

- What are the pitfalls of different approaches?

- <span class="alert">Break!</span>

- How quickly can you break a regression model? A  <span class="alert">hands-on</span> hierarchical modeling activity.

## It's that time of year.

![That November feeling...](../images/sleepy_bee.jpg)

## What are we talking about when we talk about <span class="alert">clustered data</span>?

- Repeated measures of individuals in a cohort study.

- Cross-sectional observation of flu transmission in households.

- Common social risks for non-communicable diseases in neighborhoods.

- Policies impacting cities, regions, etc (e.g. mask and vaccine mandates for COVID-19).

## Clustering may occur at multiple levels within a single dataset

![Variation in repeated measures of individuals, nested within neighrborhoods, within a city.](../images/merlo_variance_partition.png)

## Clustered data provide opportunities and challenges 

- Measure variation in effects across units.

- Adjust for unobserved or partially observed potential *confounders*.

- Predict outcomes in new units where covariates are known but outcomes are unobseved.

## Taking multiple measures of the same individual allows us to control for unobserved confounding

![Perfect setting for a within-subject case-control ðŸ‘€](../images/repeated_measure_eye.jpg)

## Making sense of <span class="alert">spatial clustering</span> often requires that we think about the environment

![John Snow's classic map of Cholera deaths in London's Golden Square in 1854](../images/snow_cholera_map.jpg)

## Clustering of <span class="alert">health behaviors</span> can impact disease risks

![Spatially clustered non-vaccination dramatically increases measles outbreak risk (Figure from Masters et al. 2020)](../images/masters_nonvax_clustering.png)

## What makes clustering such a <span class="alert">fundamental</span> epidemiological problem?

On your own, in this [google doc](https://docs.google.com/document/d/1XP-FOv3gEV2sOZVtFU970GUuIoKLVrsSyCnqDcTAc4A/edit?usp=sharing):

- What <span class="alert">mechanisms</span> make clustering happen for an applied problem you care about? 

- Identify the outcome of interest and the levels at which clustering of risk are likely to occur.

```{r}
library(countdown)
countdown(minutes = 6, warn_when = 120, seconds = 0, play_sound=TRUE)
```

## In small groups:

- Share your outcome of interest and likely mechanisms with your partners.

- How do you think <span class="alert">ignoring</span> clustering can impact conclusions we draw from data? 

- How might policies and interventions based on models that ignore clustering go wrong? Can you come up with specific examples?

```{r}
library(countdown)
countdown(minutes = 10, warn_when = 120, seconds = 0, play_sound=TRUE)
```


# A re-introduction to Generalized Linear Models (GLMs) for clustered data

## Notation for a classic un-clustered GLM

Going to be seeing a lot of this:

- $y_{i} = \alpha + \beta x_i + \epsilon_i$

\pause

Where:

 - $y_i$ is \alert{continuous} outcome measure: height, BMI, etc.
 - $\beta$ is risk associated with some kind of exposure
 - $x_i \in [0,1]$ is an indicator of exposure.
 - $\alpha$ is expected outcome when $x_i$ = 0
 - $\epsilon_i$ are independently and identically distributed (i.i.d.) errors

## Independent errors

Classic assumption is that:

- $\epsilon_i \sim N(0, \sigma^2)$

In plain-ish English:

 - Observation $y_{ij}$ of individual $i$ is a function of $\alpha + \beta x_i$ and normally distributed \alert{errors} ($\epsilon_i$) with mean zero and variance $\sigma^2$.


Another way of writing it:

- $y_i \sim N(\alpha + \beta x_i, \sigma^2)$


## Three Approaches to Modeling Clustered Data

![Which door will you choose?](../images/goat_switch.gif)


## Door #1: Ignore clustering and fit a normal GLM

- *Pool* data across all units, i.e. ignore clustering.

- i.e. fit model $y_{ij} = \alpha + \beta x_i + \epsilon_i$

- Is this a good idea? Why or why not?

## <span class="alert">NO!</span>

![Complete pooling ignores potential sources of observed and unobserved confounding.](../images/regression_dragon_1.jpg)


## Pooling clustered data violates assumption of independent errors

A <span class="alert">fully pooled</span> model:

- $y_i = \alpha + \beta x + \epsilon_i$


- $y_i$ is a combination of systematic variation ($\alpha + \beta x$) and *uncorrelated* random noise ($\epsilon_i$) where:


- $i.i.d.\ \epsilon \sim Normal(0, \sigma^2)$

## Clustering may result in correlation between average differences from mean

![Complete pooling ignores potential sources of observed and unobserved confounding.](../images/merlo_variance_partition.png)


## Your <span class="alert">residuals</span> should look like this

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width='70%', fig.cap="Example of residuals for model with clustered errors"}
require(ggplot2)
df <- data.frame(x = rnorm(1000))
g <- ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 0.1) +
  xlab("Distance from mean model prediction") +
  ylab("N")

plot(g)
```

## When you ignore clustering you may see something like:

```{r, echo=FALSE, out.width='70%', fig.cap="Clustered errors"}
require(ggplot2)
ind_cluster <- 100
ncluster <- 10
cluster_sigma <- 1
ind_sigma <- 0.2

cluster_ids <- sort(rep(1:ncluster, ind_cluster))

cluster_means <- rnorm(ncluster, sd = cluster_sigma)
ind_vals <- rnorm(n = length(cluster_ids), mean = cluster_means[cluster_ids], sd = ind_sigma)

df <- data.frame(x = ind_vals, cluster = cluster_ids)

g <- ggplot(df, aes(x = x, cluster = cluster_ids)) +
  geom_histogram(binwidth = 0.05) +
  xlab("Distance from mean") +
  ylab("N")

plot(g)
```

## Door #2: Fit a different model to each cluster

Fit *unpooled* model to each unit ($j$), assuming outcomes in each unit are independent:

- $y_{ij} = \alpha_j + \beta_j x_i + \epsilon_{ij}$

- $\epsilon_{ij} \sim N(0, \sigma_{j}^2)$

## More danger!

![Totally unpooled models run the risk of overfitting the data, particularly for small samples.](../images/regression_dino.jpg)

## Specific dangers of unpooled models

What else could go wrong here?

- Some units (e.g. counties) may have few observations, making *unpooled* models impractical

- We may want to allow some effect of exposure (e.g. having a basement) to be consistent across counties.

## Door #3: Partial Pooling!

- Allow effects to vary across clusters, but constrain them with a \alert{prior} distribution.

- This approach accommodates variation across units without assuming they have no similarity.

- More likely to make accurate \alert{out-of-sample} predictions than the fully-pooled or unpooled examples.

## Partial pooling = <span class="alert">Smoothing</span>

![Both functions fit the data perfectly...which one should you prefer and why?](../images/wiki_regularization.png)

# Break!

```{r}
library(countdown)
countdown(minutes = 5, warn_when = 60, seconds = 0, play_sound=TRUE)
```

## <span class="alert">Hands-on</span> ðŸ‘‹ with some hierarchical models

![](../images/hierarchical_app_front_page.png)

[https://sph-umich.shinyapps.io/hierarchical_models/](https://sph-umich.shinyapps.io/hierarchical_models/)

# Thanks!